{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zypNCktCxOx",
        "outputId": "1e00e724-8c02-4116-b94e-9018d908bfa2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Restricted_project\\Andi_Mandi_Shandi_iske_andar_gya_toh\\BKL_aa_hi_gye_naa\\toh_suno_TMKC\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install faiss-cpu\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from datasets import load_from_disk, Dataset\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    BertConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xzU1WtdRCxOy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class BiEncoderQA(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
        "        super().__init__()  # Correct init for modern PyTorch\n",
        "        self.clue_encoder = BertModel.from_pretrained(model_name)\n",
        "        self.answer_encoder = BertModel.from_pretrained(model_name)\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self,\n",
        "                clue_input_ids, clue_attention_mask,\n",
        "                answer_input_ids, answer_attention_mask):\n",
        "        clue_outputs = self.clue_encoder(input_ids=clue_input_ids, attention_mask=clue_attention_mask)\n",
        "        clue_emb = clue_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        answer_outputs = self.answer_encoder(input_ids=answer_input_ids, attention_mask=answer_attention_mask)\n",
        "        answer_emb = answer_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        sim_matrix = torch.matmul(clue_emb, answer_emb.transpose(0, 1))\n",
        "        targets = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n",
        "        loss = self.loss_fct(sim_matrix, targets)\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"sim_matrix\": sim_matrix,\n",
        "            \"clue_emb\": clue_emb,\n",
        "            \"answer_emb\": answer_emb,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk56lN-sGpGQ",
        "outputId": "e35d03b7-0fe3-4dfa-db86-03f44155a851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/crosswordqa_segmented.zip\n",
            "   creating: /content/crosswordqa_segmented/crosswordqa_segmented/\n",
            "  inflating: /content/crosswordqa_segmented/crosswordqa_segmented/dataset_dict.json  \n",
            "   creating: /content/crosswordqa_segmented/crosswordqa_segmented/validation/\n",
            "  inflating: /content/crosswordqa_segmented/crosswordqa_segmented/validation/data-00000-of-00001.arrow  \n",
            "  inflating: /content/crosswordqa_segmented/crosswordqa_segmented/validation/dataset_info.json  \n",
            "  inflating: /content/crosswordqa_segmented/crosswordqa_segmented/validation/state.json  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the file into a folder named 'crosswordqa_segmented'\n",
        "!unzip /content/crosswordqa_segmented.zip -d /content/crosswordqa_segmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "3cwynepfCxOy",
        "outputId": "b5cb591b-3b1a-4994-bab5-f44079500231",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "No such files: '/content/crosswordqa_segmented/crosswordqa_segmented/train/dataset_info.json', nor '/content/crosswordqa_segmented/crosswordqa_segmented/train/state.json' found. Expected to load a `Dataset` object but provided path is not a `Dataset`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7febda1096e0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset not found at: {dataset_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASETDICT_JSON_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_dict_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mdataset_dict_split_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstrip_protocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m             dataset_dict[k] = Dataset.load_from_disk(\n\u001b[0m\u001b[1;32m   1406\u001b[0m                 \u001b[0mdataset_dict_split_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1669\u001b[0m                     \u001b[0;34mf\"No such files: '{dataset_info_path}', nor '{dataset_state_json_path}' found. Expected to load a `Dataset` object, but got a `DatasetDict`. Please use either `datasets.load_from_disk` or `DatasetDict.load_from_disk` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m                 )\n\u001b[0;32m-> 1671\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1672\u001b[0m                 \u001b[0;34mf\"No such files: '{dataset_info_path}', nor '{dataset_state_json_path}' found. Expected to load a `Dataset` object but provided path is not a `Dataset`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m             )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such files: '/content/crosswordqa_segmented/crosswordqa_segmented/train/dataset_info.json', nor '/content/crosswordqa_segmented/crosswordqa_segmented/train/state.json' found. Expected to load a `Dataset` object but provided path is not a `Dataset`."
          ]
        }
      ],
      "source": [
        "# 2) Load the crosswordqa_segmented dataset from disk\n",
        "dataset_path = \"/content/crosswordqa_segmented/crosswordqa_segmented\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset not found at: {dataset_path}\")\n",
        "\n",
        "ds = load_from_disk(dataset_path)\n",
        "print(ds)\n",
        "\n",
        "# We assume ds has at least a \"train\" split, with fields: \"clue\", \"segmented_answer\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxE2GZq0CxOz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokenize_qa(batch):\n",
        "    # batch[\"clue\"] and batch[\"answer\"] are lists of strings\n",
        "    clue_encoding = tokenizer(\n",
        "        batch[\"clue\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=32\n",
        "    )\n",
        "    answer_encoding = tokenizer(\n",
        "        batch[\"answer\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=32\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"clue_input_ids\": clue_encoding[\"input_ids\"],\n",
        "        \"clue_attention_mask\": clue_encoding[\"attention_mask\"],\n",
        "        \"answer_input_ids\": answer_encoding[\"input_ids\"],\n",
        "        \"answer_attention_mask\": answer_encoding[\"attention_mask\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7SyvM5DCxOz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# We'll create a new train_dataset with \"clue\" and \"answer\" fields,\n",
        "# copying from \"clue\" and \"segmented_answer\" in the ds[\"train\"].\n",
        "def map_to_train_fields(example):\n",
        "    return {\n",
        "        \"clue\": example[\"clue\"],\n",
        "        \"answer\": example[\"segmented_answer\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "30f1939f79f44c709a839c81ee950252",
            "7805177ac597487a843cc1fd8539bb0c",
            "ffad06b5bd414676ba532957eea657ce",
            "91e2c143213f413ca3c760e50380b345",
            "5c23beac5b774a25b91a56ac93037ace",
            "13519bd423a04322a270979206269a1c",
            "27b217d137af4a31a91fc2d215603be8",
            "4ad2e0be78d64296b1bf09957545e692",
            "7b22f76c5e1d489386050b59a16b1cec",
            "9a5067acc8b04dc78390e19d13f04dfe",
            "2da496951a8447e693daaa77ee7bbd43",
            "922b529e0b17405780f78ad3f227a96a",
            "aa38594f75c140d184ca3480a0eb50a0",
            "a38576acaee84bd7aa2ce1cc1048a0cc",
            "d362eb8fc55f4fdba129e86ccc2fbd5f",
            "9240fd64b90a47b2bcd0e3d428932402",
            "85b3445c19464de6b174a9a20cb87932",
            "55067fcbcc10451a9bf9104faad5097e",
            "b0c9892b2aae4ce0a0c5f0198e9243a7",
            "278f4ffee42141e583be84f5a8d440fa",
            "8be5c0f4e2454ccfbf8793fa6717f176",
            "9c90385c7f3e4b95b65cccaed761adb1",
            "6726f23314e348bfb3ffc069493fdfb0",
            "5a8567f7b3a7451f9d0022c6fcce1946",
            "1ac03f799f05438394b3cbafa3031ef2",
            "b79b0d931eab44649667bcd1330e7a30",
            "cf0b8afb8cdf4f72a7a3d125a5ac8941",
            "52dfb73d1d3d447597f35df40bc01b30",
            "95632537b0624076b08d245a97d26232",
            "e106371e198c4032ba4e1f807dbf3909",
            "5cc5360187db4726b82da3f6f70fcfec",
            "1f493f1a654e422fa5245023caa9623d",
            "22e2c916762547c0b6add1676027ab32",
            "f7368e5e44494eb7a452acae2166c7f5",
            "75bd038242c74f2ab696b7ca8e946405",
            "18b0ac5688b543c5bdac9bfe17d98d3f",
            "1d3ff95b2f86479392e5fde402b12d0a",
            "fa8af006daeb44e0896c9b886220995d",
            "3a580991ce1b40c2a00c534a91b9b885",
            "e0501f70c205460cb5e4169510d8833f",
            "8012c352224b4d13bff2c28f3d889785",
            "30a0ec0d81654892a38d41b8e743fbfe",
            "7c1371eea322443b8ce55833d928a0d5",
            "08c45c5fd0ff4eef8171dda478217f8f"
          ]
        },
        "id": "KGHFK-xFCxOz",
        "outputId": "833bcae2-f171-490c-8abd-580c5a6c56fe",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30f1939f79f44c709a839c81ee950252",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6420790 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "922b529e0b17405780f78ad3f227a96a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6420790 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6726f23314e348bfb3ffc069493fdfb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6420133 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'clue', 'answer', 'segmented_answer', 'clue_input_ids', 'clue_attention_mask', 'answer_input_ids', 'answer_attention_mask'],\n",
            "    num_rows: 6420133\n",
            "})\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7368e5e44494eb7a452acae2166c7f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/15 shards):   0%|          | 0/6420133 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We map the training split to unify field names for tokenize_qa\n",
        "train_dataset = ds[\"train\"].map(map_to_train_fields,batched=True)\n",
        "train_dataset = train_dataset.filter(lambda x: x[\"clue\"] and x[\"answer\"])\n",
        "# Now tokenize\n",
        "train_dataset = train_dataset.map(tokenize_qa, batched=True)\n",
        "\n",
        "print(train_dataset)\n",
        "train_dataset.save_to_disk(\"train_dataset_bert_path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqpvA1qwCxO0",
        "outputId": "2b29b6ac-e4b9-443f-b888-5c879c5d57f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6420133"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = load_from_disk(\"train_dataset_bert_path\")\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (2/2 shards): 100%|██████████| 642014/642014 [04:07<00:00, 2591.37 examples/s]\n"
          ]
        }
      ],
      "source": [
        "small_train = train_dataset.train_test_split(test_size=0.10, seed=42)[\"test\"]\n",
        "small_train.save_to_disk(\"small_train_dataset10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzbsuiJDwige",
        "outputId": "8d25520f-0c02-4f26-cd6e-30693c50e3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.51.1\n",
            "Transformers path: d:\\Restricted_project\\Andi_Mandi_Shandi_iske_andar_gya_toh\\BKL_aa_hi_gye_naa\\toh_suno_TMKC\\myenv\\lib\\site-packages\\transformers\\__init__.py\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Transformers path:\", transformers.__file__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "class BiEncoderDataCollator:\n",
        "    def __call__(self, features):\n",
        "        # Convert list of dicts into dict of lists\n",
        "        batch = {\n",
        "            \"clue_input_ids\": torch.tensor([f[\"clue_input_ids\"] for f in features], dtype=torch.long),\n",
        "            \"clue_attention_mask\": torch.tensor([f[\"clue_attention_mask\"] for f in features], dtype=torch.long),\n",
        "            \"answer_input_ids\": torch.tensor([f[\"answer_input_ids\"] for f in features], dtype=torch.long),\n",
        "            \"answer_attention_mask\": torch.tensor([f[\"answer_attention_mask\"] for f in features], dtype=torch.long),\n",
        "        }\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "9ca26d39193f4bf48893b3f897da3aa5",
            "4ea8ef3089a24c85a44dcb64a4671465",
            "296c0e6ac8f84417ac617aceb3417b6f",
            "64d7773e85784edfa2c3e13ad40b2b8b",
            "c12fe4750e714b9a9558b0c5feea1308",
            "2e3f053374f749ea8cf44cfe8a11198a",
            "0550c116612e42fab30d2c4f8a81e097",
            "4cacaa5c5f984b21b2dee602fb029103",
            "a6c3fe81bae34f7ab859be4ef43b1608",
            "78ee14311c21415e8a11d1962689e49f",
            "4393dfdab8ed4c818c8625ec8881e92f",
            "7c425b3b58e94dbf94ca7cd5b5ac1397",
            "f0d71003e47340dc84176f79e6fbd959",
            "061ab3d77e304eb881fe43031268cee0",
            "a598a7c7648f4d2b93eb4b04acaf655c",
            "1a7d4c48d98c4d0690dc6f71155bfd76",
            "0e9eb11f3ea4479f9dc5527888e89e1e",
            "d035e4fc41cb4e0d96fe124d766ffb21",
            "b9192b928e2946baae4cd2dbc4580424",
            "02f23fcc9fa041819e9a4909575b0409",
            "873d7f423d1a47d7a16100d38105a402",
            "a5dc00c086194cdf8aa57476094e7cbd",
            "f55b8b29080243d0b8a334befddbe42c",
            "00406e12f21041839b5ee0ea72f830fe",
            "2b5aeeee9239473d93918a19d63d207f",
            "0725a41da75f4039980dc64ce2e1074e",
            "a64cc55ea6ae4c8b88f7e87073e64cee",
            "ef36478e6e2d4750be4d0890d1d594f7",
            "c150c33eda4f4c96a003e6dca5353c96",
            "718a01d081654683a923049f08d55678",
            "8f6a21cd8fb44c6987d4d93e73db7ae5",
            "f39d568282344207a05dac9635dcdf58",
            "3d2020d108f54b4c855e03d783a9e88a",
            "7a9522eb2c9543748c3ea5f394c91bdd",
            "8b1e1e56febe42389308ac7897c3cb66",
            "7d7c8289369a4384920a9cfad85424ed",
            "2a4a347e54144f15af60793a6ff65c07",
            "8b6ae10b4fee4c55a0b48178b24e4921",
            "e0c669dc015840588ae94fffbf2bad9f",
            "f5ff534e14db4d67886e4384ad2163ad",
            "0e7711979e504965a2a519dfbce3d938",
            "f9cf215937d04d5da6fe81a7d0c70740",
            "8b3b8c0acf924626975e4cd67010b9b2",
            "c8fda8a5d33843a5b1315ec756bb92b9",
            "c641813b6ca3483aa7bb809dcbb1ac86",
            "ab24a9776b4344e6a8d5c98def359e4e",
            "027ed76aa521457caf4f9ff18e9721d7",
            "8652c73855c84183bf6086def164cb2b",
            "22752e387a6342f78c404716df1fa916",
            "039fe22528c043f1b5867ea9f812fd7c",
            "5d7ae1d364ca4f5fa0a3ca0b2aabb7ec",
            "4c2e634fc622492da8f11a275a5d405b",
            "869d2b5f5245434cafc4308238f45953",
            "465ad038490d4b98b77e423f9069683f",
            "bbafad8cc8f549b6870454a8dea3ee69"
          ]
        },
        "id": "5GRvw50-CxO0",
        "outputId": "3f5302d1-0e18-4fba-fdb0-627ce185f5c5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ TrainingArguments initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# 4) Initialize the model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertConfig, TrainingArguments, Trainer\n",
        "model = BiEncoderQA(\"bert-base-uncased\")\n",
        "\n",
        "# 5) Set up TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bi_encoder_qa\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    eval_strategy=\"no\",  # Disable evaluation during training\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=\"./test_output\",\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     num_train_epochs=1,\n",
        "#     per_device_train_batch_size=4\n",
        "# )\n",
        "\n",
        "print(\"✅ TrainingArguments initialized successfully!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.5.1+cu121\n",
            "CUDA available: True\n",
            "CUDA version: 12.1\n",
            "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_6352\\588081313.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60189' max='60189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60189/60189 3:50:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>8.188200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.696200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.221900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.105300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.111600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.102300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.985200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.899700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.861200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.860400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.827600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.818100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.773300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.815800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.717100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.633100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.685800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.580500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.623700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.645100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.586900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.546500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.525900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.525300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.522000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.528700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.567800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>1.514100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.521800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>1.492100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>1.457700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.443100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>1.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>1.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>1.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>1.455500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.493600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>1.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>1.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>1.414100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>1.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.469200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>1.413400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>1.373200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>1.397300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>1.433300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.404700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>1.374400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>1.382200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>1.403600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>1.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.383000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>1.389800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>1.339700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>1.371600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>1.314100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.378500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>1.366900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>1.322700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>1.337200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>1.324900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.308500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>1.326100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>1.321600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>1.280700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>1.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.335000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>1.283900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>1.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>1.272700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>1.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>1.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>1.277300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>1.248300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>1.266900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>1.280200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.256900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>1.272500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>1.273900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>1.266300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>1.273200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>1.300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>1.264100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>1.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>1.186600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>1.228500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.226600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>1.272900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>1.269600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>1.204300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>1.210700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>1.257700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>1.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>1.229100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>1.211600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>1.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>1.197100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>1.207300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11300</td>\n",
              "      <td>1.195600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>1.204800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>1.190900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>1.202600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>1.176700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11800</td>\n",
              "      <td>1.193400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>1.162600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>1.150800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12100</td>\n",
              "      <td>1.184000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12200</td>\n",
              "      <td>1.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>1.147500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>1.195200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>1.146300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>1.191900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12700</td>\n",
              "      <td>1.173400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>1.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12900</td>\n",
              "      <td>1.176500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>1.128700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13100</td>\n",
              "      <td>1.129100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>1.140400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13300</td>\n",
              "      <td>1.130700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13400</td>\n",
              "      <td>1.177100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>1.108400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>1.089800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13700</td>\n",
              "      <td>1.096300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13800</td>\n",
              "      <td>1.127700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13900</td>\n",
              "      <td>1.147800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>1.142900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14100</td>\n",
              "      <td>1.131800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14200</td>\n",
              "      <td>1.114700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14300</td>\n",
              "      <td>1.162100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14400</td>\n",
              "      <td>1.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>1.129900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14600</td>\n",
              "      <td>1.049800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14700</td>\n",
              "      <td>1.043400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14800</td>\n",
              "      <td>1.100700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14900</td>\n",
              "      <td>1.120600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>1.129900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15100</td>\n",
              "      <td>1.093800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15200</td>\n",
              "      <td>1.110400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15300</td>\n",
              "      <td>1.078200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15400</td>\n",
              "      <td>1.084000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>1.088200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15600</td>\n",
              "      <td>1.123700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15700</td>\n",
              "      <td>1.050300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15800</td>\n",
              "      <td>1.095400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15900</td>\n",
              "      <td>1.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>1.090500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16100</td>\n",
              "      <td>1.087800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16200</td>\n",
              "      <td>1.084000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16300</td>\n",
              "      <td>1.061500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16400</td>\n",
              "      <td>1.100500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>1.087500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16600</td>\n",
              "      <td>1.066800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16700</td>\n",
              "      <td>1.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16800</td>\n",
              "      <td>1.046800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16900</td>\n",
              "      <td>1.084300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>1.057400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17100</td>\n",
              "      <td>1.101000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17200</td>\n",
              "      <td>0.993900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17300</td>\n",
              "      <td>1.057500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17400</td>\n",
              "      <td>1.062700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>1.037300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17600</td>\n",
              "      <td>1.082200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17700</td>\n",
              "      <td>1.063600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17800</td>\n",
              "      <td>1.072400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17900</td>\n",
              "      <td>1.092900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.991800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18100</td>\n",
              "      <td>1.035400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18200</td>\n",
              "      <td>1.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18300</td>\n",
              "      <td>1.021400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18400</td>\n",
              "      <td>1.068100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>1.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18600</td>\n",
              "      <td>1.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18700</td>\n",
              "      <td>1.054800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18800</td>\n",
              "      <td>1.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18900</td>\n",
              "      <td>1.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>1.043600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19100</td>\n",
              "      <td>1.056100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19200</td>\n",
              "      <td>1.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19300</td>\n",
              "      <td>1.017600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19400</td>\n",
              "      <td>1.053900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>1.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19600</td>\n",
              "      <td>0.996800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19700</td>\n",
              "      <td>1.091400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19800</td>\n",
              "      <td>0.996300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19900</td>\n",
              "      <td>1.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.969800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20100</td>\n",
              "      <td>0.952500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20200</td>\n",
              "      <td>0.738500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20300</td>\n",
              "      <td>0.662800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20400</td>\n",
              "      <td>0.654200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.715000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20600</td>\n",
              "      <td>0.705700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20700</td>\n",
              "      <td>0.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20800</td>\n",
              "      <td>0.672700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20900</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.683300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21100</td>\n",
              "      <td>0.705000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21200</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21300</td>\n",
              "      <td>0.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21400</td>\n",
              "      <td>0.689000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.696600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21600</td>\n",
              "      <td>0.686000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21700</td>\n",
              "      <td>0.668500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21800</td>\n",
              "      <td>0.704100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21900</td>\n",
              "      <td>0.659100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22100</td>\n",
              "      <td>0.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22200</td>\n",
              "      <td>0.707700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22300</td>\n",
              "      <td>0.701400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22400</td>\n",
              "      <td>0.683900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22600</td>\n",
              "      <td>0.702200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22700</td>\n",
              "      <td>0.677100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22800</td>\n",
              "      <td>0.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22900</td>\n",
              "      <td>0.700500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.677600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23100</td>\n",
              "      <td>0.663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23200</td>\n",
              "      <td>0.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23300</td>\n",
              "      <td>0.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23400</td>\n",
              "      <td>0.689000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.682100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23600</td>\n",
              "      <td>0.726800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23700</td>\n",
              "      <td>0.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23800</td>\n",
              "      <td>0.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23900</td>\n",
              "      <td>0.682100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24100</td>\n",
              "      <td>0.701400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24200</td>\n",
              "      <td>0.686000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24300</td>\n",
              "      <td>0.702100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24400</td>\n",
              "      <td>0.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.703200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24600</td>\n",
              "      <td>0.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24700</td>\n",
              "      <td>0.709300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24800</td>\n",
              "      <td>0.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24900</td>\n",
              "      <td>0.681900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25100</td>\n",
              "      <td>0.690900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25200</td>\n",
              "      <td>0.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25300</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25400</td>\n",
              "      <td>0.706400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.686600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25600</td>\n",
              "      <td>0.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25700</td>\n",
              "      <td>0.686300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25800</td>\n",
              "      <td>0.678700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25900</td>\n",
              "      <td>0.691700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.675200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26100</td>\n",
              "      <td>0.680900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26200</td>\n",
              "      <td>0.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26300</td>\n",
              "      <td>0.685100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26400</td>\n",
              "      <td>0.668100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.706800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26600</td>\n",
              "      <td>0.671000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26700</td>\n",
              "      <td>0.657100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26800</td>\n",
              "      <td>0.715400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26900</td>\n",
              "      <td>0.693600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.691100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27100</td>\n",
              "      <td>0.639500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27200</td>\n",
              "      <td>0.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27300</td>\n",
              "      <td>0.681500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27400</td>\n",
              "      <td>0.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27600</td>\n",
              "      <td>0.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27700</td>\n",
              "      <td>0.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27800</td>\n",
              "      <td>0.690600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27900</td>\n",
              "      <td>0.654200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.677700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28100</td>\n",
              "      <td>0.641600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28200</td>\n",
              "      <td>0.650700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28300</td>\n",
              "      <td>0.645200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28400</td>\n",
              "      <td>0.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.668000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28600</td>\n",
              "      <td>0.667900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28700</td>\n",
              "      <td>0.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28800</td>\n",
              "      <td>0.667100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28900</td>\n",
              "      <td>0.625200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.686600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29100</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29200</td>\n",
              "      <td>0.671400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29300</td>\n",
              "      <td>0.642600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29400</td>\n",
              "      <td>0.657600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.684000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29600</td>\n",
              "      <td>0.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29700</td>\n",
              "      <td>0.699600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29800</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29900</td>\n",
              "      <td>0.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.638100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30100</td>\n",
              "      <td>0.632600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30200</td>\n",
              "      <td>0.684900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30300</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30400</td>\n",
              "      <td>0.665300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30600</td>\n",
              "      <td>0.661400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30700</td>\n",
              "      <td>0.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30800</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30900</td>\n",
              "      <td>0.721900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.644500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31100</td>\n",
              "      <td>0.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31200</td>\n",
              "      <td>0.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31300</td>\n",
              "      <td>0.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31400</td>\n",
              "      <td>0.665800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31600</td>\n",
              "      <td>0.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31700</td>\n",
              "      <td>0.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31800</td>\n",
              "      <td>0.615500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31900</td>\n",
              "      <td>0.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.656400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32100</td>\n",
              "      <td>0.630000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32200</td>\n",
              "      <td>0.655000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32300</td>\n",
              "      <td>0.657800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32400</td>\n",
              "      <td>0.686800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32600</td>\n",
              "      <td>0.643600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32700</td>\n",
              "      <td>0.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32800</td>\n",
              "      <td>0.656100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32900</td>\n",
              "      <td>0.626400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.637900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33100</td>\n",
              "      <td>0.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33200</td>\n",
              "      <td>0.641800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33300</td>\n",
              "      <td>0.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33400</td>\n",
              "      <td>0.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.645100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33600</td>\n",
              "      <td>0.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33700</td>\n",
              "      <td>0.643600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33800</td>\n",
              "      <td>0.616300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33900</td>\n",
              "      <td>0.642200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.620800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34100</td>\n",
              "      <td>0.618100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34200</td>\n",
              "      <td>0.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34300</td>\n",
              "      <td>0.657500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34400</td>\n",
              "      <td>0.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.626000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34600</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34700</td>\n",
              "      <td>0.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34800</td>\n",
              "      <td>0.650800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34900</td>\n",
              "      <td>0.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.617800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35100</td>\n",
              "      <td>0.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35200</td>\n",
              "      <td>0.653500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35300</td>\n",
              "      <td>0.648900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35400</td>\n",
              "      <td>0.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.656100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35600</td>\n",
              "      <td>0.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35700</td>\n",
              "      <td>0.671100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35800</td>\n",
              "      <td>0.646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35900</td>\n",
              "      <td>0.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36100</td>\n",
              "      <td>0.572000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36200</td>\n",
              "      <td>0.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36300</td>\n",
              "      <td>0.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36400</td>\n",
              "      <td>0.637400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36600</td>\n",
              "      <td>0.554300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36700</td>\n",
              "      <td>0.607900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36800</td>\n",
              "      <td>0.626100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36900</td>\n",
              "      <td>0.646000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.617300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37100</td>\n",
              "      <td>0.612700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37200</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37300</td>\n",
              "      <td>0.636200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37400</td>\n",
              "      <td>0.629000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.604800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37600</td>\n",
              "      <td>0.634300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37700</td>\n",
              "      <td>0.620700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37800</td>\n",
              "      <td>0.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37900</td>\n",
              "      <td>0.609000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38100</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38200</td>\n",
              "      <td>0.618300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38300</td>\n",
              "      <td>0.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38400</td>\n",
              "      <td>0.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.649700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38600</td>\n",
              "      <td>0.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38700</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38800</td>\n",
              "      <td>0.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38900</td>\n",
              "      <td>0.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.614900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39100</td>\n",
              "      <td>0.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39200</td>\n",
              "      <td>0.605900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39300</td>\n",
              "      <td>0.614600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39400</td>\n",
              "      <td>0.590200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.595500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39600</td>\n",
              "      <td>0.599400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39700</td>\n",
              "      <td>0.593800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39800</td>\n",
              "      <td>0.604800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39900</td>\n",
              "      <td>0.597600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40100</td>\n",
              "      <td>0.585500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40200</td>\n",
              "      <td>0.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40300</td>\n",
              "      <td>0.378800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40400</td>\n",
              "      <td>0.343500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>0.344800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40600</td>\n",
              "      <td>0.339700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40700</td>\n",
              "      <td>0.374100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40800</td>\n",
              "      <td>0.355000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40900</td>\n",
              "      <td>0.368400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.373500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41100</td>\n",
              "      <td>0.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41200</td>\n",
              "      <td>0.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41300</td>\n",
              "      <td>0.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41400</td>\n",
              "      <td>0.382700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>0.359300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41600</td>\n",
              "      <td>0.359600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41700</td>\n",
              "      <td>0.357400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41800</td>\n",
              "      <td>0.347300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41900</td>\n",
              "      <td>0.333400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.370600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42100</td>\n",
              "      <td>0.345700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42200</td>\n",
              "      <td>0.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42300</td>\n",
              "      <td>0.358300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42400</td>\n",
              "      <td>0.362500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>0.340500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42600</td>\n",
              "      <td>0.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42700</td>\n",
              "      <td>0.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42800</td>\n",
              "      <td>0.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42900</td>\n",
              "      <td>0.340500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>0.362900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43100</td>\n",
              "      <td>0.364200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43200</td>\n",
              "      <td>0.365400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43300</td>\n",
              "      <td>0.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43400</td>\n",
              "      <td>0.351400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>0.353800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43600</td>\n",
              "      <td>0.398300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43700</td>\n",
              "      <td>0.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43800</td>\n",
              "      <td>0.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43900</td>\n",
              "      <td>0.382300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>0.362500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44100</td>\n",
              "      <td>0.385400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44200</td>\n",
              "      <td>0.376200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44300</td>\n",
              "      <td>0.335600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44400</td>\n",
              "      <td>0.373700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>0.411900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44600</td>\n",
              "      <td>0.355500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44700</td>\n",
              "      <td>0.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44800</td>\n",
              "      <td>0.348800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44900</td>\n",
              "      <td>0.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>0.366500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45100</td>\n",
              "      <td>0.348900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45200</td>\n",
              "      <td>0.340500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45300</td>\n",
              "      <td>0.342900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45400</td>\n",
              "      <td>0.366800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>0.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45600</td>\n",
              "      <td>0.376000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45700</td>\n",
              "      <td>0.368900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45800</td>\n",
              "      <td>0.357100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45900</td>\n",
              "      <td>0.334100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>0.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46100</td>\n",
              "      <td>0.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46200</td>\n",
              "      <td>0.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46300</td>\n",
              "      <td>0.351500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46400</td>\n",
              "      <td>0.368900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>0.369300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46600</td>\n",
              "      <td>0.360800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46700</td>\n",
              "      <td>0.370500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46800</td>\n",
              "      <td>0.351300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46900</td>\n",
              "      <td>0.366000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>0.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47100</td>\n",
              "      <td>0.359000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47200</td>\n",
              "      <td>0.329200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47300</td>\n",
              "      <td>0.332400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47400</td>\n",
              "      <td>0.354400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>0.352800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47600</td>\n",
              "      <td>0.341800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47700</td>\n",
              "      <td>0.347800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47800</td>\n",
              "      <td>0.371000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47900</td>\n",
              "      <td>0.373800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>0.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48100</td>\n",
              "      <td>0.369900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48200</td>\n",
              "      <td>0.358200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48300</td>\n",
              "      <td>0.372000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48400</td>\n",
              "      <td>0.353600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>0.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48600</td>\n",
              "      <td>0.356400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48700</td>\n",
              "      <td>0.357200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48800</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48900</td>\n",
              "      <td>0.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.351800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49100</td>\n",
              "      <td>0.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49200</td>\n",
              "      <td>0.374400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49300</td>\n",
              "      <td>0.323100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49400</td>\n",
              "      <td>0.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>0.368900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49600</td>\n",
              "      <td>0.346100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49700</td>\n",
              "      <td>0.335300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49800</td>\n",
              "      <td>0.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49900</td>\n",
              "      <td>0.366800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.351400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50100</td>\n",
              "      <td>0.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50200</td>\n",
              "      <td>0.357300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50300</td>\n",
              "      <td>0.359300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50400</td>\n",
              "      <td>0.345300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>0.325300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50600</td>\n",
              "      <td>0.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50700</td>\n",
              "      <td>0.316900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50800</td>\n",
              "      <td>0.305500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50900</td>\n",
              "      <td>0.333200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>0.303800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51100</td>\n",
              "      <td>0.354700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51200</td>\n",
              "      <td>0.329200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51300</td>\n",
              "      <td>0.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51400</td>\n",
              "      <td>0.316800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>0.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51600</td>\n",
              "      <td>0.347100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51700</td>\n",
              "      <td>0.340600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51800</td>\n",
              "      <td>0.349500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51900</td>\n",
              "      <td>0.362500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>0.346600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52100</td>\n",
              "      <td>0.333300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52200</td>\n",
              "      <td>0.331800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52300</td>\n",
              "      <td>0.344400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52400</td>\n",
              "      <td>0.317600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>0.339600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52600</td>\n",
              "      <td>0.327100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52700</td>\n",
              "      <td>0.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52800</td>\n",
              "      <td>0.308300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52900</td>\n",
              "      <td>0.347000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>0.349800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53100</td>\n",
              "      <td>0.348900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53200</td>\n",
              "      <td>0.312200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53300</td>\n",
              "      <td>0.339900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53400</td>\n",
              "      <td>0.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>0.321300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53600</td>\n",
              "      <td>0.349900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53700</td>\n",
              "      <td>0.331500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53800</td>\n",
              "      <td>0.324200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53900</td>\n",
              "      <td>0.349800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>0.332400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54100</td>\n",
              "      <td>0.320900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54200</td>\n",
              "      <td>0.342300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54300</td>\n",
              "      <td>0.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54400</td>\n",
              "      <td>0.337000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>0.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54600</td>\n",
              "      <td>0.305300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54700</td>\n",
              "      <td>0.337400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54800</td>\n",
              "      <td>0.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54900</td>\n",
              "      <td>0.340500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>0.322300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55100</td>\n",
              "      <td>0.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55200</td>\n",
              "      <td>0.353300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55300</td>\n",
              "      <td>0.325600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55400</td>\n",
              "      <td>0.360400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>0.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55600</td>\n",
              "      <td>0.320100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55700</td>\n",
              "      <td>0.292300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55800</td>\n",
              "      <td>0.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55900</td>\n",
              "      <td>0.322600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.340800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56100</td>\n",
              "      <td>0.312900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56200</td>\n",
              "      <td>0.300900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56300</td>\n",
              "      <td>0.334400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56400</td>\n",
              "      <td>0.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>0.368400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56600</td>\n",
              "      <td>0.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56700</td>\n",
              "      <td>0.331600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56800</td>\n",
              "      <td>0.283900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56900</td>\n",
              "      <td>0.346700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>0.324900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57100</td>\n",
              "      <td>0.324100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57200</td>\n",
              "      <td>0.324700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57300</td>\n",
              "      <td>0.311800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57400</td>\n",
              "      <td>0.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>0.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57600</td>\n",
              "      <td>0.313200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57700</td>\n",
              "      <td>0.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57800</td>\n",
              "      <td>0.334700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57900</td>\n",
              "      <td>0.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58100</td>\n",
              "      <td>0.340100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58200</td>\n",
              "      <td>0.331800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58300</td>\n",
              "      <td>0.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58400</td>\n",
              "      <td>0.307100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>0.337700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58600</td>\n",
              "      <td>0.319400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58700</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58800</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58900</td>\n",
              "      <td>0.341300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>0.338500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59100</td>\n",
              "      <td>0.299400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59200</td>\n",
              "      <td>0.316800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59300</td>\n",
              "      <td>0.328400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59400</td>\n",
              "      <td>0.346700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>0.310600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59600</td>\n",
              "      <td>0.300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59700</td>\n",
              "      <td>0.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59800</td>\n",
              "      <td>0.335000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59900</td>\n",
              "      <td>0.317000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>0.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60100</td>\n",
              "      <td>0.305200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60189, training_loss=0.7824161881419253, metrics={'train_runtime': 13857.668, 'train_samples_per_second': 138.987, 'train_steps_per_second': 4.343, 'total_flos': 0.0, 'train_loss': 0.7824161881419253, 'epoch': 3.0})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 6) Initialize Trainer\n",
        "small_train=load_from_disk(\"small_train_dataset10\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train,\n",
        "    data_collator=BiEncoderDataCollator(), \n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "# 7) Train the model (Optional)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to bi_encoder_qa_model\n"
          ]
        }
      ],
      "source": [
        "save_path=\"bi_encoder_qa_model\"\n",
        "model.clue_encoder.save_pretrained(f\"{save_path}/clue_encoder\")\n",
        "model.answer_encoder.save_pretrained(f\"{save_path}/answer_encoder\")\n",
        "print(f\"Model saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer saved to bi_encoder_qa_model/tokenizer\n"
          ]
        }
      ],
      "source": [
        "tokenizer.save_pretrained(f\"({save_path}/tokenizer\")\n",
        "print(f\"Tokenizer saved to {save_path}/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded and moved to device: cuda\n",
            "✅ Loaded 437721 answers.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class BiEncoderQA(torch.nn.Module):\n",
        "    def __init__(self, clue_encoder=None, answer_encoder=None):\n",
        "        super().__init__()\n",
        "        self.clue_encoder = clue_encoder or BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.answer_encoder = answer_encoder or BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.loss_fct = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, clue_input_ids, clue_attention_mask, answer_input_ids, answer_attention_mask):\n",
        "        clue_emb = self.clue_encoder(clue_input_ids, clue_attention_mask).last_hidden_state[:, 0, :]\n",
        "        answer_emb = self.answer_encoder(answer_input_ids, answer_attention_mask).last_hidden_state[:, 0, :]\n",
        "        sim_matrix = torch.matmul(clue_emb, answer_emb.transpose(0, 1))\n",
        "        targets = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n",
        "        loss = self.loss_fct(sim_matrix, targets)\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"sim_matrix\": sim_matrix,\n",
        "            \"clue_emb\": clue_emb,\n",
        "            \"answer_emb\": answer_emb,\n",
        "        }\n",
        "\n",
        "\n",
        "# Load the encoders\n",
        "clue_encoder = BertModel.from_pretrained(\"bi_encoder_qa_model/clue_encoder\")\n",
        "answer_encoder = BertModel.from_pretrained(\"bi_encoder_qa_model/answer_encoder\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bi_encoder_qa_model/tokenizer\")\n",
        "\n",
        "# Initialize model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BiEncoderQA(clue_encoder, answer_encoder).to(device)\n",
        "\n",
        "# Print model to confirm it's loaded correctly\n",
        "print(f\"Model loaded and moved to device: {device}\")\n",
        "\n",
        "# Load the answer set file (the same one used during training)\n",
        "with open(\"answer_set_segmented.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    file_answers = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"✅ Loaded {len(file_answers)} answers.\")\n",
        "\n",
        "# Function to compute embeddings for answers in batches\n",
        "def compute_answer_embeddings(answer_list, model, tokenizer, device=\"cuda\", batch_size=16):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(answer_list), batch_size), desc=\"Embedding answers\"):\n",
        "        batch = answer_list[i:i+batch_size]\n",
        "        encoded = tokenizer(batch, padding=True, truncation=True, max_length=32, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model.answer_encoder(**encoded)\n",
        "                batch_emb = output.last_hidden_state[:, 0, :]  # [CLS] embedding\n",
        "        all_embeddings.append(batch_emb.cpu().float().numpy())\n",
        "    return np.vstack(all_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BiEncoderQA(clue_encoder, answer_encoder).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding answers:   0%|          | 0/27358 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_11380\\1953318426.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Embedding answers: 100%|██████████| 27358/27358 [04:25<00:00, 102.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FAISS index built with 437721 entries.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "\n",
        "# Now import faiss and run normally\n",
        "import faiss\n",
        "\n",
        "# Compute answer embeddings\n",
        "answer_embeddings = compute_answer_embeddings(file_answers, model, tokenizer, device=device)\n",
        "\n",
        "# Build FAISS index for cosine similarity\n",
        "dim = answer_embeddings.shape[1]\n",
        "faiss.normalize_L2(answer_embeddings)\n",
        "faiss_index = faiss.IndexFlatIP(dim)\n",
        "faiss_index.add(answer_embeddings)\n",
        "\n",
        "print(f\"✅ FAISS index built with {faiss_index.ntotal} entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'faiss_index' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m test_clue \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapital of France\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m true_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m predicted_answers \u001b[38;5;241m=\u001b[39m answer_for_clue(test_clue, model, tokenizer, \u001b[43mfaiss_index\u001b[49m, file_answers, device\u001b[38;5;241m=\u001b[39mdevice, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     18\u001b[0m found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ans \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predicted_answers, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# print(f\"{i}. {ans}\")\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'faiss_index' is not defined"
          ]
        }
      ],
      "source": [
        "# Inference function (search for top-k answers given a clue)\n",
        "def answer_for_clue(clue, model, tokenizer, faiss_index, answer_list, device=\"cuda\", k=10):\n",
        "    model.eval()\n",
        "    encoded = tokenizer(clue, return_tensors=\"pt\", truncation=True, padding=True, max_length=32).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.clue_encoder(**encoded)\n",
        "        clue_emb = output.last_hidden_state[:, 0, :]\n",
        "    clue_emb = clue_emb.cpu().numpy()\n",
        "    faiss.normalize_L2(clue_emb)\n",
        "    distances, indices = faiss_index.search(clue_emb, k)\n",
        "    return [answer_list[i] for i in indices[0]]\n",
        "\n",
        "# Test with a clue\n",
        "test_clue = \"Capital of France\"\n",
        "true_answer = \"Paris\"\n",
        "predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers, device=device, k=1000)\n",
        "\n",
        "found = False\n",
        "for i, ans in enumerate(predicted_answers, 1):\n",
        "    # print(f\"{i}. {ans}\")\n",
        "    if ans.strip().lower() == true_answer.strip().lower():\n",
        "        print(f\"\\n🎯 Match found at rank {i}!\")\n",
        "        found = True\n",
        "\n",
        "if not found:\n",
        "    print(\"\\n❌ Correct answer not found in top-k predictions.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "faiss.write_index(faiss_index, \"faiss_index.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "faiss_index = faiss.read_index(\"faiss_index.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Restricted_project\\Andi_Mandi_Shandi_iske_andar_gya_toh\\BKL_aa_hi_gye_naa\\toh_suno_TMKC\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Full test size: 361458\n",
            "✅ 10% test subset size: 36146\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the full dataset\n",
        "ds = load_from_disk(\"crosswordqa_segmented\")\n",
        "\n",
        "# Take 10% of the test set\n",
        "test_full = ds[\"validation\"]\n",
        "test_small = test_full.train_test_split(test_size=0.10, seed=42)[\"test\"]\n",
        "\n",
        "print(f\"✅ Full test size: {len(test_full)}\")\n",
        "print(f\"✅ 10% test subset size: {len(test_small)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_small' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      4\u001b[0m topk_hits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_small\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(test_small, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Top-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      8\u001b[0m     clue \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_small' is not defined"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "k = 100\n",
        "topk_hits = 0\n",
        "total = len(test_small)\n",
        "\n",
        "for example in tqdm(test_small, desc=f\"Evaluating Top-{k} Accuracy\"):\n",
        "    clue = example[\"clue\"]\n",
        "    true_answer = example[\"segmented_answer\"].strip().lower()\n",
        "\n",
        "    predicted = answer_for_clue(clue, model, tokenizer, faiss_index, file_answers, device=device, k=k)\n",
        "    predicted_clean = [p.strip().lower() for p in predicted]\n",
        "\n",
        "    if true_answer in predicted_clean:\n",
        "        topk_hits += 1\n",
        "\n",
        "topk_accuracy = topk_hits / total\n",
        "print(f\"\\n✅ Top-{k} Accuracy on {total} test examples: {topk_accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 437721 answers\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_answers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m answers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Function to embed answers in batches\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Load the answer list\n",
        "with open(\"answer_set_segmented.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    file_answers = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"✅ Loaded {len(file_answers)} answers\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to embed answers in batches\n",
        "def compute_answer_embeddings(answer_list, model, tokenizer, device=\"cuda\", batch_size=1024):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(answer_list), batch_size), desc=\"Embedding answers\"):\n",
        "        batch = answer_list[i:i+batch_size]\n",
        "        encoded = tokenizer(batch, padding=True, truncation=True, max_length=32, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model.answer_encoder(**encoded)\n",
        "                batch_emb = output.last_hidden_state[:, 0, :]\n",
        "        all_embeddings.append(batch_emb.cpu().float().numpy())\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Generate answer embeddings\n",
        "answer_embeddings = compute_answer_embeddings(file_answers, model, tokenizer, device=device)\n",
        "\n",
        "# Build FAISS index for cosine similarity\n",
        "dim = answer_embeddings.shape[1]\n",
        "faiss.normalize_L2(answer_embeddings)\n",
        "faiss_index = faiss.IndexFlatIP(dim)\n",
        "faiss_index.add(answer_embeddings)\n",
        "print(f\"✅ FAISS index built with {faiss_index.ntotal} entries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload later\n",
        "from transformers import BertModel, BertTokenizer\n",
        "clue_encoder = BertModel.from_pretrained(\"bi_encoder_model/clue_encoder\")\n",
        "answer_encoder = BertModel.from_pretrained(\"bi_encoder_model/answer_encoder\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bi_encoder_model/tokenizer\")\n",
        "\n",
        "model = BiEncoderQA(clue_encoder, answer_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Load saved FAISS index\n",
        "faiss_index = faiss.read_index(\"faiss_index.bin\")\n",
        "\n",
        "# Load the answer list\n",
        "with open(\"answer_set_segmented.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    file_answers = [line.strip() for line in f if line.strip()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_for_clue(clue, model, tokenizer, faiss_index, answer_list, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", k=3):\n",
        "    model.eval()\n",
        "    encoded = tokenizer(clue, return_tensors=\"pt\", truncation=True, padding=True, max_length=32).to(device)\n",
        "    with torch.no_grad():\n",
        "        clue_output = model.clue_encoder(**encoded)\n",
        "        clue_emb = clue_output.last_hidden_state[:, 0, :]  # CLS token\n",
        "        clue_emb = clue_emb.cpu().numpy()\n",
        "    # Normalize for cosine similarity\n",
        "    faiss.normalize_L2(clue_emb)\n",
        "    distances, indices = faiss_index.search(clue_emb, k)\n",
        "    return [answer_list[i] for i in indices[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_clue = \"Capital of France\"\n",
        "predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers)\n",
        "print(\"Clue:\", test_clue)\n",
        "print(\"Top predictions:\", predicted_answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "LEWU6vbrCxO0",
        "outputId": "07acc735-f1f0-4bab-aad2-58679de3e7f4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 437721 answers from /content/answer_set_no_segment.txt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-38ecb0aea694>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Compute embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0manswer_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_answer_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-38ecb0aea694>\u001b[0m in \u001b[0;36mcompute_answer_embeddings\u001b[0;34m(answer_list, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 ).to(device)\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 8) Inference with FAISS (like the original example)\n",
        "#    We assume you built a large answer_set file (answer_set_segmented.txt).\n",
        "#    We'll just show how to embed & index it.\n",
        "\n",
        "answer_file = \"/content/answer_set_no_segment.txt\"\n",
        "if not os.path.exists(answer_file):\n",
        "    print(f\"❌ {answer_file} not found. Please provide a valid path.\")\n",
        "else:\n",
        "    # Load your large answer set from file\n",
        "    with open(answer_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_answers = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"✅ Loaded {len(file_answers)} answers from {answer_file}\")\n",
        "\n",
        "    # Function to compute embeddings for answers\n",
        "    def compute_answer_embeddings(answer_list, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        model.eval()\n",
        "        embeddings = []\n",
        "        with torch.no_grad():\n",
        "            for ans in answer_list:\n",
        "                encoded = tokenizer(\n",
        "                    ans,\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True,\n",
        "                    padding=True,\n",
        "                    max_length=32\n",
        "                ).to(device)\n",
        "                outputs = model.answer_encoder(**encoded)\n",
        "                emb = outputs.last_hidden_state[:, 0, :]\n",
        "                embeddings.append(emb.cpu().numpy())\n",
        "        # shape: (num_answers, hidden_dim)\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Compute embeddings\n",
        "    answer_embeddings = compute_answer_embeddings(file_answers, model, tokenizer, device=device)\n",
        "    dim = answer_embeddings.shape[1]\n",
        "\n",
        "    # Build FAISS index for cosine similarity\n",
        "    faiss.normalize_L2(answer_embeddings)\n",
        "    faiss_index = faiss.IndexFlatIP(dim)\n",
        "    faiss_index.add(answer_embeddings)\n",
        "    print(f\"✅ Built FAISS index with {faiss_index.ntotal} embeddings.\")\n",
        "\n",
        "# Inference function\n",
        "    def answer_for_clue(clue, model, tokenizer, faiss_index, answer_list, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", k=3):\n",
        "        model.eval()\n",
        "        encoded = tokenizer(clue, return_tensors=\"pt\", truncation=True, padding=True, max_length=32).to(device)\n",
        "        with torch.no_grad():\n",
        "            clue_output = model.clue_encoder(**encoded)\n",
        "            clue_emb = clue_output.last_hidden_state[:, 0, :]\n",
        "            clue_emb = clue_emb.cpu().numpy()\n",
        "        # Normalize clue for cosine\n",
        "        faiss.normalize_L2(clue_emb)\n",
        "        distances, indices = faiss_index.search(clue_emb, k)\n",
        "        candidates = [answer_list[idx] for idx in indices[0]]\n",
        "        return candidates\n",
        "\n",
        "    # Test\n",
        "    test_clue = \"What is the capital of France?\"\n",
        "    predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers, device=device, k=3)\n",
        "    print(\"Clue:\", test_clue)\n",
        "    print(\"Candidate Answers:\", predicted_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOhkzootzMkU",
        "outputId": "81b685cb-5839-429f-ed2f-55366fac125b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 437721 answers from /content/answer_set_no_segment.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEmbedding answers:   0%|          | 0/107 [00:00<?, ?it/s]<ipython-input-11-0bd540ea0bdf>:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Embedding answers: 100%|██████████| 107/107 [02:06<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Built FAISS index with 437721 embeddings.\n",
            "Clue: What is the capital of France?\n",
            "Candidate Answers: ['what is sorare', 'what is the title of', 'what is the name for a']\n"
          ]
        }
      ],
      "source": [
        "# 8) Inference with FAISS (like the original example)\n",
        "#    We assume you built a large answer_set file (answer_set_segmented.txt).\n",
        "#    We'll just show how to embed & index it.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import BertModel, BertTokenizer, TrainingArguments, Trainer\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "answer_file = \"/content/answer_set_no_segment.txt\"\n",
        "if not os.path.exists(answer_file):\n",
        "    print(f\"❌ {answer_file} not found. Please provide a valid path.\")\n",
        "else:\n",
        "    # Load your large answer set from file\n",
        "    with open(answer_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_answers = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"✅ Loaded {len(file_answers)} answers from {answer_file}\")\n",
        "\n",
        "    # Function to compute embeddings for answers with progress tracking\n",
        "    def compute_answer_embeddings(answer_list, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", batch_size=4096):\n",
        "      \"\"\"\n",
        "      Compute embeddings in batches, with mixed precision.\n",
        "      batch_size=4096 is a good starting point; increase it until you\n",
        "      fill ~12–14 GB of your 16 GB GPU.\n",
        "      \"\"\"\n",
        "      model.eval()\n",
        "      # Cast model to half precision for faster inference & lower memory\n",
        "      model.answer_encoder.half()\n",
        "\n",
        "      all_embs = []\n",
        "      with torch.no_grad():\n",
        "          for i in tqdm(range(0, len(answer_list), batch_size), desc=\"Embedding answers\"):\n",
        "              batch = answer_list[i : i + batch_size]\n",
        "              # Tokenize batch: dynamic pad to max in batch\n",
        "              encoded = tokenizer(\n",
        "                  batch,\n",
        "                  return_tensors=\"pt\",\n",
        "                  truncation=True,\n",
        "                  padding=True,      # pad to longest in this batch\n",
        "                  max_length=32\n",
        "              ).to(device)\n",
        "\n",
        "              # Mixed‑precision inference\n",
        "              with torch.cuda.amp.autocast():\n",
        "                  outputs = model.answer_encoder(**encoded)\n",
        "                  # take CLS token\n",
        "                  emb = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "              # Move to CPU and convert to numpy\n",
        "              all_embs.append(emb.cpu().float().numpy())\n",
        "\n",
        "      # Stack into (N, D)\n",
        "      return np.vstack(all_embs)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Compute embeddings\n",
        "    answer_embeddings = compute_answer_embeddings(file_answers, model, tokenizer, device=device)\n",
        "    dim = answer_embeddings.shape[1]\n",
        "\n",
        "    # Build FAISS index for cosine similarity\n",
        "    faiss.normalize_L2(answer_embeddings)  # normalize embeddings for cosine similarity\n",
        "    faiss_index = faiss.IndexFlatIP(dim)\n",
        "    faiss_index.add(answer_embeddings)  # adds embeddings to the FAISS index\n",
        "    print(f\"✅ Built FAISS index with {faiss_index.ntotal} embeddings.\")\n",
        "\n",
        "    # Inference function remains the same\n",
        "    def answer_for_clue(clue, model, tokenizer, faiss_index, answer_list, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", k=3):\n",
        "        model.eval()\n",
        "        encoded = tokenizer(clue, return_tensors=\"pt\", truncation=True, padding=True, max_length=32).to(device)\n",
        "        with torch.no_grad():\n",
        "            clue_output = model.clue_encoder(**encoded)\n",
        "            clue_emb = clue_output.last_hidden_state[:, 0, :]  # CLS token\n",
        "            clue_emb = clue_emb.cpu().numpy()\n",
        "        # Normalize clue embedding for cosine similarity\n",
        "        faiss.normalize_L2(clue_emb)\n",
        "        distances, indices = faiss_index.search(clue_emb, k)\n",
        "        # Retrieve top k answer candidates using the indices\n",
        "        candidates = [answer_list[idx] for idx in indices[0]]\n",
        "        return candidates\n",
        "\n",
        "    # Test inference with a sample clue\n",
        "    test_clue = \"What is the capital of France?\"\n",
        "    predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers, device=device, k=3)\n",
        "    print(\"Clue:\", test_clue)\n",
        "    print(\"Candidate Answers:\", predicted_answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1ci47JDM38Rh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class BatchEvaluator:\n",
        "    def __init__(self, model, tokenizer, faiss_index, file_answers, device, batch_size=16, k=10):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.faiss_index = faiss_index\n",
        "        self.file_answers = file_answers\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.k = k\n",
        "\n",
        "        # Try to inspect the model's embedding function to use it directly\n",
        "        # This avoids the issue with forward() needing both query and answer\n",
        "        if hasattr(model, 'get_clue_embeddings'):\n",
        "            print(\"Using model's get_clue_embeddings method\")\n",
        "            self.embed_function = model.get_clue_embeddings\n",
        "        elif hasattr(model, 'encode_query'):\n",
        "            print(\"Using model's encode_query method\")\n",
        "            self.embed_function = model.encode_query\n",
        "        else:\n",
        "            print(\"No specific embedding function found, will extract from model directly\")\n",
        "            self.embed_function = None\n",
        "\n",
        "    def get_embeddings(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Extract embeddings directly from the CLM or encoder model component\"\"\"\n",
        "        # Check if the model has a specific clue encoder component\n",
        "        if hasattr(self.model, 'clue_encoder'):\n",
        "            encoder = self.model.clue_encoder\n",
        "            outputs = encoder(input_ids, attention_mask=attention_mask)\n",
        "            return outputs.last_hidden_state[:, 0, :]\n",
        "        # Or check if it has a query encoder\n",
        "        elif hasattr(self.model, 'query_encoder'):\n",
        "            encoder = self.model.query_encoder\n",
        "            outputs = encoder(input_ids, attention_mask=attention_mask)\n",
        "            return outputs.last_hidden_state[:, 0, :]\n",
        "        # Otherwise try to access the backbone model directly\n",
        "        elif hasattr(self.model, 'transformer'):\n",
        "            encoder = self.model.transformer\n",
        "            outputs = encoder(input_ids, attention_mask=attention_mask)\n",
        "            return outputs.last_hidden_state[:, 0, :]\n",
        "        else:\n",
        "            # No direct encoder access, return None to indicate we need to use the forward method\n",
        "            return None\n",
        "\n",
        "    def process_single_clue(self, clue: str):\n",
        "        \"\"\"Process a single clue\"\"\"\n",
        "        # Tokenize the query input\n",
        "        inputs = self.tokenizer(clue, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            # Extract input tensors\n",
        "            query_input_ids = inputs['input_ids']\n",
        "            query_attention_mask = inputs.get('attention_mask', None)\n",
        "\n",
        "            # First try to use a dedicated embedding function\n",
        "            if self.embed_function is not None:\n",
        "                if query_attention_mask is not None:\n",
        "                    query_emb = self.embed_function(query_input_ids, query_attention_mask)\n",
        "                else:\n",
        "                    query_emb = self.embed_function(query_input_ids)\n",
        "            else:\n",
        "                # Try to get embeddings directly from the encoder component\n",
        "                query_emb = self.get_embeddings(query_input_ids, query_attention_mask)\n",
        "\n",
        "                # If that doesn't work, we need to bypass the forward method\n",
        "                if query_emb is None:\n",
        "                    # Inspect model to find encoders\n",
        "                    if hasattr(self.model, 'clue_encoder') and self.model.clue_encoder is not None:\n",
        "                        # Use the clue encoder directly\n",
        "                        outputs = self.model.clue_encoder(query_input_ids, attention_mask=query_attention_mask)\n",
        "                        query_emb = outputs.last_hidden_state[:, 0, :]\n",
        "                    else:\n",
        "                        # Create dummy answer inputs with matching dtype\n",
        "                        # Get the dtype of the query inputs\n",
        "                        input_dtype = query_input_ids.dtype\n",
        "\n",
        "                        # Create dummy answer inputs matching the dtype\n",
        "                        answer_input_ids = torch.zeros((1, 1), dtype=input_dtype).to(self.device)\n",
        "                        answer_attention_mask = torch.ones((1, 1), dtype=input_dtype).to(self.device)\n",
        "\n",
        "                        try:\n",
        "                            # Try to access internal model attributes to get embeddings directly\n",
        "                            # This avoids the similarity calculation that's causing the dtype error\n",
        "\n",
        "                            # Save the original forward method\n",
        "                            original_forward = self.model.forward\n",
        "\n",
        "                            # Define a custom forward that just returns the embeddings\n",
        "                            def custom_forward(self_model, clue_input_ids, clue_attention_mask, answer_input_ids, answer_attention_mask):\n",
        "                                # Get the clue encoder outputs\n",
        "                                clue_outputs = self_model.clue_encoder(\n",
        "                                    clue_input_ids,\n",
        "                                    attention_mask=clue_attention_mask\n",
        "                                )\n",
        "                                # Return just the clue embeddings\n",
        "                                return clue_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "                            # Replace the forward method temporarily\n",
        "                            self.model.forward = custom_forward.__get__(self.model, type(self.model))\n",
        "\n",
        "                            # Call the model with our custom forward\n",
        "                            query_emb = self.model(\n",
        "                                query_input_ids,\n",
        "                                query_attention_mask,\n",
        "                                answer_input_ids,\n",
        "                                answer_attention_mask\n",
        "                            )\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting embeddings: {e}\")\n",
        "                            # If the custom approach fails, print a detailed model structure\n",
        "                            print(f\"Model structure: {dir(self.model)}\")\n",
        "                            raise\n",
        "                        finally:\n",
        "                            # Restore the original forward method\n",
        "                            self.model.forward = original_forward\n",
        "\n",
        "        # Convert to numpy for FAISS - ensure it's the right shape\n",
        "        if len(query_emb.shape) == 3:  # [batch, seq_len, hidden]\n",
        "            query_emb = query_emb[:, 0, :]  # Take CLS token\n",
        "\n",
        "        query_vector = query_emb.cpu().numpy()\n",
        "\n",
        "        # Search in FAISS index\n",
        "        scores, indices = self.faiss_index.search(query_vector, self.k)\n",
        "\n",
        "        # Get the actual answers\n",
        "        predictions = [self.file_answers[idx] for idx in indices[0]]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def process_batch(self, clues: List[str]) -> List[List[str]]:\n",
        "        \"\"\"Process clues one by one\"\"\"\n",
        "        all_predictions = []\n",
        "\n",
        "        # Process each clue individually\n",
        "        for clue in clues:\n",
        "            predictions = self.process_single_clue(clue)\n",
        "            all_predictions.append(predictions)\n",
        "\n",
        "        return all_predictions\n",
        "\n",
        "    def evaluate(self, validation_set):\n",
        "        \"\"\"Evaluate the model on the entire validation set\"\"\"\n",
        "        total_examples = len(validation_set)\n",
        "\n",
        "        # Initialize counters\n",
        "        top1_hits = 0\n",
        "        topk_hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # Create progress bar\n",
        "        pbar = tqdm(total=total_examples, desc=\"Evaluating\")\n",
        "        pbar.set_postfix_str(f\"Top-1: 0.00% | Top-{self.k}: 0.00%\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        last_update_time = start_time\n",
        "\n",
        "        # Process the dataset in batches\n",
        "        for i in range(0, total_examples, self.batch_size):\n",
        "            # Get batch indices\n",
        "            end_idx = min(i + self.batch_size, total_examples)\n",
        "            batch = validation_set.select(range(i, end_idx))\n",
        "\n",
        "            # Extract clues and answers\n",
        "            batch_clues = batch[\"clue\"]\n",
        "            batch_true_answers = [ans.strip().lower() for ans in batch[\"segmented_answer\"]]\n",
        "\n",
        "            # Get predictions for all clues in batch\n",
        "            batch_preds = self.process_batch(batch_clues)\n",
        "\n",
        "            # Process results\n",
        "            for true_answer, preds in zip(batch_true_answers, batch_preds):\n",
        "                preds_clean = [p.strip().lower() for p in preds]\n",
        "\n",
        "                total += 1\n",
        "                if true_answer == preds_clean[0]:\n",
        "                    top1_hits += 1\n",
        "                if true_answer in preds_clean:\n",
        "                    topk_hits += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.update(len(batch))\n",
        "\n",
        "            # Update metrics display more frequently\n",
        "            current_time = time.time()\n",
        "            if total % 100 == 0 or (current_time - last_update_time) > 5:\n",
        "                top1_acc_current = top1_hits / total\n",
        "                topk_acc_current = topk_hits / total\n",
        "                elapsed = current_time - start_time\n",
        "                examples_per_second = total / elapsed if elapsed > 0 else 0\n",
        "                eta = (total_examples - total) / examples_per_second if examples_per_second > 0 else 0\n",
        "\n",
        "                pbar.set_postfix_str(\n",
        "                    f\"Top-1: {top1_acc_current:.2%} | Top-{self.k}: {topk_acc_current:.2%} | \"\n",
        "                    f\"Speed: {examples_per_second:.1f} ex/s | ETA: {eta/60:.1f}m\"\n",
        "                )\n",
        "                last_update_time = current_time\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # Final accuracy results\n",
        "        top1_acc = top1_hits / total\n",
        "        topk_acc = topk_hits / total\n",
        "\n",
        "        return {\n",
        "            \"top1_accuracy\": top1_acc,\n",
        "            \"topk_accuracy\": topk_acc,\n",
        "            \"k\": self.k,\n",
        "            \"total_examples\": total\n",
        "        }\n",
        "    def inspect_model(self):\n",
        "      \"\"\"Print detailed model information to help debug\"\"\"\n",
        "      print(\"\\nModel Information:\")\n",
        "      print(f\"Model type: {type(self.model)}\")\n",
        "      print(f\"Model attributes: {dir(self.model)}\")\n",
        "\n",
        "      # Try to access encoder components\n",
        "      if hasattr(self.model, 'clue_encoder'):\n",
        "          print(f\"Clue encoder type: {type(self.model.clue_encoder)}\")\n",
        "      if hasattr(self.model, 'answer_encoder'):\n",
        "          print(f\"Answer encoder type: {type(self.model.answer_encoder)}\")\n",
        "\n",
        "      # Check if model has specific embedding methods\n",
        "      for method in ['get_clue_embeddings', 'encode_query', 'get_query_embeddings']:\n",
        "          if hasattr(self.model, method):\n",
        "              print(f\"Model has {method} method\")\n",
        "\n",
        "# Add this method to the BatchEvaluator class\n",
        "# Call it in the __init__ method to debug model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mmcKKtDEACka"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1RfM_ak-o_l",
        "outputId": "531dad60-5f70-4922-ba47-d8efb23ce014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No specific embedding function found, will extract from model directly\n",
            "\n",
            "Model Information:\n",
            "Model type: <class '__main__.BiEncoderQA'>\n",
            "Model attributes: ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'answer_encoder', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'clue_encoder', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'loss_fct', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "Clue encoder type: <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
            "Answer encoder type: <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
            "Test clue: Clothing items sold by TomboyX\n",
            "Test result: ['flea bags', 'trading cards', 'home accessories', 'litter box liners', 'dog toys', 'claw footwear', 'giant pets', 'playsuits', 'toy animals', 'item in a golf bag']\n",
            "Expected: bras\n"
          ]
        }
      ],
      "source": [
        "# Create evaluator instance\n",
        "evaluator = BatchEvaluator(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    faiss_index=faiss_index,\n",
        "    file_answers=file_answers,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    k=10\n",
        ")\n",
        "\n",
        "# Add this debugging step\n",
        "evaluator.inspect_model()\n",
        "\n",
        "# Test with a single example first before running full evaluation\n",
        "test_clue = val_set[0][\"clue\"]\n",
        "test_result = evaluator.process_single_clue(test_clue)\n",
        "print(f\"Test clue: {test_clue}\")\n",
        "print(f\"Test result: {test_result}\")\n",
        "print(f\"Expected: {val_set[0]['segmented_answer']}\")\n",
        "\n",
        "# If the test is successful, run the full evaluation\n",
        "# results = evaluator.evaluate(val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2Cbbfblu6aw",
        "outputId": "f21cf250-97fc-4bf3-b2ca-68709673fca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 437721 answers from answer_set_segmented.txt\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack(all_embeddings)\n\u001b[0;32m     43\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Compute embeddings in batches\u001b[39;00m\n\u001b[0;32m     47\u001b[0m answer_embeddings \u001b[38;5;241m=\u001b[39m compute_answer_embeddings(file_answers, model, tokenizer, device\u001b[38;5;241m=\u001b[39mdevice, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "answer_file = \"answer_set_segmented.txt\"\n",
        "if not os.path.exists(answer_file):\n",
        "    print(f\"❌ {answer_file} not found. Please provide a valid path.\")\n",
        "else:\n",
        "    # Load your large answer set from file\n",
        "    with open(answer_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_answers = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"✅ Loaded {len(file_answers)} answers from {answer_file}\")\n",
        "\n",
        "    # Function to compute embeddings for answers in batches with mixed precision\n",
        "    def compute_answer_embeddings(answer_list, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", batch_size=64):\n",
        "        model.eval()\n",
        "        all_embeddings = []\n",
        "        # Process in large batches\n",
        "        for i in tqdm(range(0, len(answer_list), batch_size), desc=\"Embedding answers\"):\n",
        "            batch_answers = answer_list[i:i+batch_size]\n",
        "            # Tokenize batch: dynamic padding to the longest example in the batch\n",
        "            encoded = tokenizer(\n",
        "                batch_answers,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=32\n",
        "            ).to(device)\n",
        "\n",
        "            # Use mixed precision (remove device_type, just set dtype)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                outputs = model.answer_encoder(**encoded)\n",
        "                batch_emb = outputs.last_hidden_state[:, 0, :]  # [CLS] token embedding\n",
        "\n",
        "            # Move batch embeddings to CPU and convert to float32\n",
        "            all_embeddings.append(batch_emb.cpu().float().detach().numpy())\n",
        "        return np.vstack(all_embeddings)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Compute embeddings in batches\n",
        "    answer_embeddings = compute_answer_embeddings(file_answers, model, tokenizer, device=device, batch_size=8)\n",
        "    dim = answer_embeddings.shape[1]\n",
        "\n",
        "    # Build FAISS index for cosine similarity\n",
        "    faiss.normalize_L2(answer_embeddings)\n",
        "    faiss_index = faiss.IndexFlatIP(dim)\n",
        "    faiss_index.add(answer_embeddings)\n",
        "    print(f\"✅ Built FAISS index with {faiss_index.ntotal} embeddings.\")\n",
        "\n",
        "    # Inference function remains the same:\n",
        "    def answer_for_clue(clue, model, tokenizer, faiss_index, answer_list, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", k=3):\n",
        "        model.eval()\n",
        "        encoded = tokenizer(clue, return_tensors=\"pt\", truncation=True, padding=True, max_length=32).to(device)\n",
        "        with torch.no_grad():\n",
        "            clue_output = model.clue_encoder(**encoded)\n",
        "            clue_emb = clue_output.last_hidden_state[:, 0, :]  # CLS token\n",
        "            clue_emb = clue_emb.cpu().numpy()\n",
        "        # Normalize clue embedding for cosine similarity\n",
        "        faiss.normalize_L2(clue_emb)\n",
        "        distances, indices = faiss_index.search(clue_emb, k)\n",
        "        candidates = [answer_list[idx] for idx in indices[0]]\n",
        "        return candidates\n",
        "\n",
        "    # Test inference with a sample clue\n",
        "    test_clue = \"What is the capital of France?\"\n",
        "    predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers, device=device, k=3)\n",
        "    print(\"Clue:\", test_clue)\n",
        "    print(\"Candidate Answers:\", predicted_answers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmAkYF7k3o_G",
        "outputId": "b08e5565-8121-43ae-8d0d-f4a0632b1da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FAISS index saved to faiss_index.bin\n"
          ]
        }
      ],
      "source": [
        "faiss.write_index(faiss_index, \"faiss_index.bin\")\n",
        "print(\"✅ FAISS index saved to faiss_index.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "G46vKfrs1dKh",
        "outputId": "7b09d825-be95-4fa1-c4ef-750f7b5facc0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'embeddings_file' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a675c3b90c00>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Saved embeddings to {embeddings_file}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embeddings_file' is not defined"
          ]
        }
      ],
      "source": [
        "np.save(embeddings_file, answer_embeddings)\n",
        "print(f\"✅ Saved embeddings to {embeddings_file}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2xlkkskzuv2",
        "outputId": "79e2b3ad-01fb-4446-eb65-40ecdd83b48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clue: Milky gemstones\n",
            "Candidate Answers: ['gemstones', 'precious gems', 'rare gems', 'sapphires', 'precious stones', 'witch hazels', 'pipestones', 'egg beakers', 'stone sandsticks', 'crystal gazers', 'iron pellets', 'fairy cakes', 'gallic stones', 'diamondearrings', 'curling stones', 'garnets', 'ribalds', 'lodestones', 'dilithium crystals', 'glassshards', 'korn flakes', 'woodenware', 'holy stones', 'rare metals', 'tea cakes', 'tinfoils', 'precious metals', 'capstones', 'prisms', 'jewel boxes', 'tilestones', 'dyeweeds', 'granites', 'adozen redroses', 'power teals', 'beet sandpieces', 'emeralds', 'ice pellets', 'brawnflakes', 'mints', 'tapioca pearls', 'microbursts', 'green clovers', 'honeycombs', 'naval oranges', 'watchbands', 'golden raisins', 'wheelhorses', 'pearly gametes', 'water melons', 'poultices', 'mineral spirits', 'lotuses', 'talismans', 'nut shells', 'fruit preserves', 'friedonion rings', 'robin sandroses', 'stinkweeds', 'crescent moons', 'choice morsels', 'candy canes', 'poetic gems', 'magic charms', 'plumcots', 'tea leaves', 'sea dragons', 'tin stone', 'nutcases', 'pink teas', 'rare stones', 'graphites', 'gollum stones', 'precious metal', 'marbles', 'mangonels', 'gold bricks', 'thin mints', 'yeast cakes', 'teawagons', 'spice cakes', 'rubies', 'glassblowers', 'crysour grapes', 'cowintheteens', 'curling stone', 'needlepoint', 'meteors', 'lava rocks', 'magnific ambersons', 'bugle beads', 'moonstones', 'hollowware', 'seamy serpents', 'aesopsfoibles', 'coppers', 'welsh dressers', 'nectarines', 'fish eyelenses', 'shoebills', 'circlets', 'corncob pipes', 'water melonseeds', 'water melonrinds', 'healing stones', 'coffins', 'ironweeds', 'flatwares', 'pokeweeds', 'long steedroses', 'iolites', 'myrmidons', 'bramble jelly', 'great seals', 'circulars', 'milometers', 'pumice stone', 'gumweeds', 'pellets', 'barbells', 'teabiscuits', 'easter seals', 'spirit animals', 'dyeware', 'gullets', 'gilds', 'prayer beads', 'quartzes', 'doll houses', 'fruit cakes', 'giftware', 'polehorses', 'millenaries', 'whalebone', 'honey cakes', 'sharkskins', 'pheasants eyes', 'chocolate bars', 'acepots', 'cigarets', 'damasks', 'blue chipstocks', 'pollen baskets', 'earth shakers', 'wheels of cheese', 'tupper warelids', 'safe crackers', 'ironware', 'bell ringers', 'graphite', 'herbal medicines', 'rattans', 'wax museums', 'purslanes', 'christmas seals', 'gemmes', 'snake stone', 'bell lyres', 'chicken flingers', 'cockleshells', 'honey dews', 'cross pieces', 'serpents', 'sugar lumps', 'cornrows', 'enamelware', 'sunbursts', 'catseye marbles', 'pineapple rings', 'ticktocks', 'bamboos', 'crown jewels', 'stones stone', 'hollow earthers', 'crayolas', 'neonbulbs', 'radishes', 'barbets', 'blue moonshiners', 'chhetrums', 'soothsayers', 'fire irons', 'lime wedges', 'creches', 'bonbons', 'lotus eaters', 'water diviners', 'tepees', 'skilodges', 'trilemmas', 'jack hammers', 'refracted colors', 'citrines', 'stoneware', 'razorbills', 'calamines', 'tinware', 'holly mosses', 'moonshinemakers', 'slates', 'pendants', 'beryls', 'trivets', 'cherry pickers', 'diamond studs', 'lattice bloomers', 'sponges', 'doggerels', 'cloverleafs', 'claret cups', 'tamoshanters', 'honeypots', 'precious gem', 'gluebills', 'colorforglasses', 'tiars', 'brassies', 'shirehorses', 'garnet namer', 'water colors', 'chapsticks', 'poppies', 'penlites', 'clam shells', 'silver belles', 'amethysts', 'trefoils', 'kidney stone', 'straw dogs', 'lockshorns', 'jewels fifer', 'hallmarks', 'pinochles', 'ether muffs', 'jewelers', 'ice cubes', 'reed pipes', 'resins', 'wigmakers', 'nickels', 'ceruleans', 'phalanges', 'imintheweeds', 'precious stone', 'champagne grapes', 'gemses', 'inkpots', 'basket cases', 'rimrock', 'trace elements', 'apple seeds', 'glass houses', 'giant pets', 'lemon soles', 'barnflakes', 'sand baghogs', 'elephant seals', 'locoweeds', 'anonymous sauces', 'brassware', 'dairies', 'communion cups', 'lollards', 'brass rings', 'tangerines', 'dye pots', 'quinines', 'studhorses', 'sunspots', 'cigar bars', 'retiles', 'liqueur glass', 'caskets', 'grails', 'goosecrows', 'sdnaalwood trees', 'french furies', 'moondials', 'marmites', 'black cherries', 'banana peels', 'brooms', 'sweet wines', 'magic wands', 'dice cups', 'silver spoons', 'distilled spirit', 'cornish flakes', 'icebergs', 'sour doughs', 'perfumescents', 'livable waxes', 'micrometeorite', 'punglasses', 'tin amelts', 'metalware', 'mossroses', 'fairy tales', 'shatterers', 'shinsplints', 'wineskins', 'chocolateeclairs', 'plugged nickelplates', 'avowals', 'bangles and beads', 'sachets', 'opal pickles', 'steel woolpads', 'shardboiled eggs', 'riveters', 'emporiums', 'weavershitches', 'granite ware', 'bonmots', 'medallions', 'sunflower seeds', 'saltpans', 'diamondsofnines', 'gelcaps', 'enamelers', 'eared seals', 'rubbers', 'water hollies', 'bluish wines', 'corn flakes', 'gluepots', 'classical motifs', 'waffle irons', 'puzzle rings', 'tundramelts', 'specialbulletins', 'chocolate cakes', 'nutcrackers', 'mineral salt', 'rillets', 'corn crakes', 'muskmelons', 'jellybeans', 'lapidaries', 'chicaneries', 'nut bars', 'copper cups', 'computer cursers', 'genies', 'silvered almonds', 'terrariums', 'branimal crackers', 'crayon fish', 'egg cartons', 'fish eyelens', 'caddos', 'jade stone', 'juice bars', 'animal spirits', 'angeood cakes', 'stone poneys', 'pentagram', 'seed cakes', 'sandpipers', 'solariums', 'semi preciousetons', 'rare fybans', 'egalites', 'onyx cameos', 'china pinks', 'sandworts', 'pentaglots', 'acetones', 'scarabs', 'montana sapphires', 'florals', 'bookmarkers', 'lemondroppies', 'concoctions', 'labyrinths', 'clockwatchers', 'terracottatiles', 'uncut gems', 'steel spirals', 'carcases', 'minimuffins', 'carthorses', 'snowblades', 'roman candles', 'clovers', 'tombolas', 'rutiles', 'sadsacks', 'sand wedges', 'dna strands', 'penny pinchers', 'macadams', 'banana republics', 'mineral salts', 'trigrams', 'eddystone rocks', 'pantiles', 'monkey flowers', 'cowries', 'stupas', 'widowsweeds', 'chevrons', 'superbowl ring', 'gentians', 'fairy stories', 'waxmakers', 'algreen cards', 'sidebets', 'crystalline lens', 'maltesers', 'mahaleb cherries', 'gastraps', 'boxed candies', 'lamas', 'applepies', 'sunwatchers', 'summoners', 'carnations', 'mandarin oranges', 'atopancakes', 'hypnotics', 'checkstubs', 'tin alloys', 'magpies', 'tea berries', 'maglites', 'dustbins', 'love tokens', 'oranges', 'woodlice', 'beaked hazelnuts', 'solar tiles', 'opal ring', 'glassslippers', 'black cauldron', 'trinkets', 'simnel cakes', 'nemonic devices', 'apricot brand', 'spinneries', 'ice scrapers', 'clock towers', 'tiny daleevans', 'shticks', 'mystics', 'ming vases', 'nutn honey', 'jordan almonds', 'stearines', 'fire stone', 'bacardibreezers', 'morsels', 'almonds', 'mandarins', 'magic clippers', 'follicles', 'incenses', 'brownies', 'jeweled crones', 'myrrhmaids', 'monster hams', 'vinesodajam nuts', 'goldfish ponder', 'nimbus clouds', 'micrometeorites', 'crowbars', 'sugar cubes', 'bird songs', 'gumbos', 'pentangles', 'mason jars', 'numetal bands', 'teakettles', 'redstones', 'silver smiths', 'acorn flakes', 'book tokens', 'trace material', 'oil colors', 'gemstone', 'mosaics', 'nougats', 'peales', 'gourds', 'lumineers', 'pangrams', 'duck feathers', 'tangrams', 'trollies', 'crescents', 'horse feathers', 'broad bovines', 'deerlet', 'diamond minxes', 'rose quartz', 'royal poincianas', 'cupid cakes', 'mustardplasters', 'caroches', 'peapods', 'corundum diamond', 'fiveeasypieces', 'bowbells', 'chimney pieces', 'canny openers', 'sealions', 'rocking stones', 'crab pots', 'eyefluids', 'vortices', 'poppy seeds', 'confects', 'barbands', 'rock caves', 'dollarbillets', 'canneries', 'hoplites', 'moonshiners', 'salt domes', 'anil dye', 'willow shell', 'red sauces', 'thymes', 'pickerels', 'clingingvines', 'whiskeydrinkers', 'chestnuthorses', 'marmosets', 'corks', 'ice axes', 'paper mulberries', 'madegrees', 'norse gods', 'cornisens', 'teapots', 'defilers', 'chilblains', 'candygrams', 'privets', 'pots of gold', 'mermaids', 'lamina crackers', 'diviners', 'iridiums', 'reddiaperbabies', 'lockesmiths', 'cyclamates', 'hashers', 'pink houses', 'cera cookware', 'hanging gardens', 'tamarisks', 'teletypes', 'smelts', 'hlograms', 'klipspringers', 'fruits of', 'raisines', 'bee hives', 'teeboxes', 'sorcerers stone', 'maidenhair fern', 'tangry men', 'can openers', 'sponge cakes', 'kerosene lamps', 'tea oil', 'applestoapples', 'camel backs', 'dna molecules', 'systyles', 'triplicates', 'beetrees', 'sallows', 'psychic phenomena', 'snowblowers', 'iron maidens', 'ice fogs', 'herb teas', 'almond cookies', 'sick beads inseat', 'enamel melodies', 'grand pappies', 'verifiers', 'baby cakes', 'sulphurs', 'zodiacs', 'ice makers', 'afewbadapples', 'electric razors', 'beef medallions', 'apple squares', 'woolblankets', 'oblations', 'mohairs', 'city slicers', 'embossments', 'novelettes', 'currer bells', 'atomic clogs', 'egg shells', 'scepters', 'sapsuckers', 'itchweeds', 'venetian glass', 'buntings', 'fairy tallies', 'witches caldrons', 'scowels', 'apple polishers', 'cinnamons', 'elephant seal', 'man flakes', 'shinguards', 'poisoners', 'peridots', 'tricolors', 'garlands', 'korans', 'thistles', 'ftowels', 'string of pearls', 'brassards', 'walnuts', 'beakers', 'chocolates', 'petroleum jelly', 'princess dairies', 'pearls of wisdom', 'searats', 'egg boxes', 'crockpots', 'stone stone', 'lushes', 'gypsums', 'stone houses', 'stainless steels', 'burets', 'paragons', 'baby foods', 'champagne corks', 'chalcedonies', 'aravorite flower', 'sorcerers', 'woodchippers', 'candy stores', 'dirt magnets', 'kettlebells', 'stoat cakes', 'iatethe bones', 'incense organs', 'lipcreams', 'egg fruit', 'firecrackers', 'heralds', 'snow globes', 'currants', 'long stemroses', 'zoomorphs', 'igloos', 'conflakes', 'oyster mushrooms', 'binderies', 'pasterns', 'bathoils', 'necromancers', 'rose coloredglasses', 'amulets', 'dogsitters', 'medicinal oil', 'onion rings', 'teasquares', 'embroidery floss', 'weevils', 'baby kittens', 'flat irons', 'banana cakes', 'breadpans', 'drierclimes', 'flatware', 'unicorn flakes', 'stick figures', 'manxmassplains', 'teastains', 'seal rings', 'minorcan areas', 'mercurys', 'fire bars', 'ghost dusters', 'spindle shanks', 'lions tooth', 'hexagrams', 'beaglewithcream cheese', 'calicotrees', 'bone china', 'foeaf clovers', 'sparky scrapers', 'dogroses', 'sages', 'satines', 'condiments', 'aerolites', 'suspicious mints', 'sigils', 'dipswitches', 'apple sandoranges', 'calomels', 'beauty icons', 'jellyfish', 'molds', 'bridal shoers', 'mastabas', 'golden fleas', 'wine makers', 'eoliths', 'oopsallberries', 'loose teas', 'baby bloomers', 'yellow quartz', 'trivia buffs', 'nile greens', 'anagrams', 'cipher texts', 'oniontarts', 'diadembones', 'meat cakes', 'prosand crayons', 'quatrefoils', 'knackers', 'stollen quiches', 'watermarks', 'solderers', 'adviaries', 'lemon pipers', 'flower girls', 'gift horses', 'snowbells', 'claridges', 'pieces of ajigsaw puzzle', 'lakes of', 'meattrays', 'crow quills', 'sharp objects', 'vermicelli bowl', 'thorn trims', 'sterling silvers', 'wigwams', 'green teas', 'pitch stone', 'moth pieces', 'glassware', 'horsepens', 'tarot pear', 'condiment jars', 'soap stone', 'waxbeans', 'beemen', 'crab morsels', 'gascons', 'giblets', 'pilseners', 'feather pllows', 'delicacies', 'carrsellers', 'rain pipes', 'pigments', 'stem winders', 'treasure mapmark', 'stirrers', 'micrometeors', 'grate cheese', 'incandescent lampblack', 'common housepets', 'honey trees', 'elysian fields', 'chambars', 'rainbow colors', 'crusts', 'daisy chains', 'thermistors', 'earwigs', 'boffins', 'silver birches', 'mint marks', 'pound cakes', 'neoliths', 'shoe knaves', 'goatscape', 'star stone', 'alfalfas', 'taborets', 'ztiles', 'fire ants', 'satin dolls', 'foal feathers', 'wands', 'inkhorns', 'sprite bottles', 'lanolins', 'shiraz wine', 'fruit juice jelly', 'horse hairs', 'merlins', 'silverdollarpancakes', 'sparrowgullscardinals', 'seers', 'crudites', 'traces of nuts', 'addaxes', 'cellphonecharms', 'unicorns', 'ellulite', 'banchory', 'carpet beggars', 'silkweed', 'teahouses', 'apple pixies', 'hotwheels', 'tea urns', 'pindots', 'pinweeds', 'yakof diamonds', 'edenites', 'gabblers', 'virgins', 'peacocks', 'peanutshells', 'cocks combs', 'cherry colas', 'iron stone', 'tunicates', 'synthetics', 'longjohn silversparrot', 'pet projects', 'jacarandas', 'tea trees', 'armorers', 'tins', 'maindiagonals', 'fish cakes', 'starcharts', 'penises', 'cockles', 'headshrinkers', 'goatherds', 'bten flakes', 'quartztimepiece', 'windlasses', 'dromedaries', 'chinooks', 'cherries', 'meadow mushrooms', 'angelas gashes', 'irish mosses', 'iserines', 'funnel cakes', 'sugar bakers', 'laserpointers', 'hammers', 'fineries', 'donkeymen', 'pitch pipes', 'fugees', 'doxies', 'soap cakes', 'gift tokens', 'madrigals', 'nephrons', 'green acers', 'pot plants', 'shacks', 'cream teas', 'rosins', 'irishbrogues', 'fiddlesticks', 'future tins', 'bean potts', 'mosses', 'irregulars', 'salterns', 'gated cheese', 'pectines', 'octopuss', 'eustace diamonds', 'marble cakes', 'ringlets', 'innards', 'cuboids', 'brass nameplates', 'french duffs', 'sea caves', 'rainhats', 'cashew sandcolas', 'biblical plaques', 'knitwear', 'entrees', 'veal cutlets', 'iolite', 'refried beans', 'daisy chain', 'algae green', 'sesame seeds', 'pagans', 'beer coasters', 'osteins', 'flower children', 'simnel cake', 'opalines', 'eardrops', 'pipefitters', 'gaulstones', 'eclairs', 'nervines', 'fire bricks', 'footsthebills', 'arrowheads', 'lavenders', 'morgues', 'folk rock', 'old coins', 'ebonites', 'cotton bowls', 'zirconmines', 'clays', 'potters wheels', 'grannysmiths', 'tartlets', 'lea collars', 'mead larks', 'tinders', 'cigar bands', 'litterbox liners', 'billsbills', 'harebells', 'binarystarrs', 'orbs', 'sunstones', 'honeys', 'dallasweeds', 'wedding bands', 'rhesus cups', 'vampers', 'dustbunnies', 'shagreens', 'coinpurses', 'liquorature', 'sealabs', 'dimes', 'banana cluster', 'pawners', 'cola nuts', 'marms', 'cultured pearls', 'trumpet vines', 'rice wine', 'french barrettes', 'rabbit pears', 'perfume bottle', 'drill bits', 'lenscaps', 'wisdoms', 'window envelopes', 'identical tins', 'green chipstocks', 'rock tumblers', 'thecairngorms', 'crayon debergerac', 'pearlite', 'recipe tins', 'jof diamonds', 'faiences', 'agate marble', 'tarpot', 'hexameters', 'monoliths', 'house kippers', 'bluebells', 'cheese heads', 'spinetnails', 'altar cloths', 'bitone clefs', 'treasure hunts', 'stitchings', 'haematite', 'forest canes', 'teati melosers', 'soft goods', 'dyeweed', 'cannon copiers', 'watermelon sugar', 'recess pieces', 'snow flowers', 'fleabane', 'diamond mines', 'bazars', 'giant eaters', 'caves in', 'junior mints', 'emblems', 'funneweel cakes']\n"
          ]
        }
      ],
      "source": [
        "# Test inference with a sample clue\n",
        "test_clue = \"Milky gemstones\"\n",
        "predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers, device=device, k=1000)\n",
        "print(\"Clue:\", test_clue)\n",
        "print(\"Candidate Answers:\", predicted_answers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00406e12f21041839b5ee0ea72f830fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef36478e6e2d4750be4d0890d1d594f7",
            "placeholder": "​",
            "style": "IPY_MODEL_c150c33eda4f4c96a003e6dca5353c96",
            "value": "tokenizer.json: 100%"
          }
        },
        "027ed76aa521457caf4f9ff18e9721d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c2e634fc622492da8f11a275a5d405b",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_869d2b5f5245434cafc4308238f45953",
            "value": 440449768
          }
        },
        "02f23fcc9fa041819e9a4909575b0409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "039fe22528c043f1b5867ea9f812fd7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0550c116612e42fab30d2c4f8a81e097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "061ab3d77e304eb881fe43031268cee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9192b928e2946baae4cd2dbc4580424",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02f23fcc9fa041819e9a4909575b0409",
            "value": 231508
          }
        },
        "0725a41da75f4039980dc64ce2e1074e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f39d568282344207a05dac9635dcdf58",
            "placeholder": "​",
            "style": "IPY_MODEL_3d2020d108f54b4c855e03d783a9e88a",
            "value": " 466k/466k [00:00&lt;00:00, 5.84MB/s]"
          }
        },
        "08c45c5fd0ff4eef8171dda478217f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7711979e504965a2a519dfbce3d938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e9eb11f3ea4479f9dc5527888e89e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13519bd423a04322a270979206269a1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18b0ac5688b543c5bdac9bfe17d98d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8012c352224b4d13bff2c28f3d889785",
            "max": 6420133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30a0ec0d81654892a38d41b8e743fbfe",
            "value": 6420133
          }
        },
        "1a7d4c48d98c4d0690dc6f71155bfd76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac03f799f05438394b3cbafa3031ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e106371e198c4032ba4e1f807dbf3909",
            "max": 6420133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cc5360187db4726b82da3f6f70fcfec",
            "value": 6420133
          }
        },
        "1d3ff95b2f86479392e5fde402b12d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c1371eea322443b8ce55833d928a0d5",
            "placeholder": "​",
            "style": "IPY_MODEL_08c45c5fd0ff4eef8171dda478217f8f",
            "value": " 6420133/6420133 [00:12&lt;00:00, 508144.92 examples/s]"
          }
        },
        "1f493f1a654e422fa5245023caa9623d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22752e387a6342f78c404716df1fa916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e2c916762547c0b6add1676027ab32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "278f4ffee42141e583be84f5a8d440fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27b217d137af4a31a91fc2d215603be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "296c0e6ac8f84417ac617aceb3417b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cacaa5c5f984b21b2dee602fb029103",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6c3fe81bae34f7ab859be4ef43b1608",
            "value": 48
          }
        },
        "2a4a347e54144f15af60793a6ff65c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3b8c0acf924626975e4cd67010b9b2",
            "placeholder": "​",
            "style": "IPY_MODEL_c8fda8a5d33843a5b1315ec756bb92b9",
            "value": " 570/570 [00:00&lt;00:00, 15.4kB/s]"
          }
        },
        "2b5aeeee9239473d93918a19d63d207f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_718a01d081654683a923049f08d55678",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f6a21cd8fb44c6987d4d93e73db7ae5",
            "value": 466062
          }
        },
        "2da496951a8447e693daaa77ee7bbd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e3f053374f749ea8cf44cfe8a11198a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a0ec0d81654892a38d41b8e743fbfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30f1939f79f44c709a839c81ee950252": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7805177ac597487a843cc1fd8539bb0c",
              "IPY_MODEL_ffad06b5bd414676ba532957eea657ce",
              "IPY_MODEL_91e2c143213f413ca3c760e50380b345"
            ],
            "layout": "IPY_MODEL_5c23beac5b774a25b91a56ac93037ace"
          }
        },
        "3a580991ce1b40c2a00c534a91b9b885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2020d108f54b4c855e03d783a9e88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4393dfdab8ed4c818c8625ec8881e92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "465ad038490d4b98b77e423f9069683f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad2e0be78d64296b1bf09957545e692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c2e634fc622492da8f11a275a5d405b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cacaa5c5f984b21b2dee602fb029103": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ea8ef3089a24c85a44dcb64a4671465": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e3f053374f749ea8cf44cfe8a11198a",
            "placeholder": "​",
            "style": "IPY_MODEL_0550c116612e42fab30d2c4f8a81e097",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "52dfb73d1d3d447597f35df40bc01b30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55067fcbcc10451a9bf9104faad5097e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8567f7b3a7451f9d0022c6fcce1946": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52dfb73d1d3d447597f35df40bc01b30",
            "placeholder": "​",
            "style": "IPY_MODEL_95632537b0624076b08d245a97d26232",
            "value": "Map: 100%"
          }
        },
        "5c23beac5b774a25b91a56ac93037ace": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc5360187db4726b82da3f6f70fcfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d7ae1d364ca4f5fa0a3ca0b2aabb7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64d7773e85784edfa2c3e13ad40b2b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ee14311c21415e8a11d1962689e49f",
            "placeholder": "​",
            "style": "IPY_MODEL_4393dfdab8ed4c818c8625ec8881e92f",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.01kB/s]"
          }
        },
        "6726f23314e348bfb3ffc069493fdfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a8567f7b3a7451f9d0022c6fcce1946",
              "IPY_MODEL_1ac03f799f05438394b3cbafa3031ef2",
              "IPY_MODEL_b79b0d931eab44649667bcd1330e7a30"
            ],
            "layout": "IPY_MODEL_cf0b8afb8cdf4f72a7a3d125a5ac8941"
          }
        },
        "718a01d081654683a923049f08d55678": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75bd038242c74f2ab696b7ca8e946405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a580991ce1b40c2a00c534a91b9b885",
            "placeholder": "​",
            "style": "IPY_MODEL_e0501f70c205460cb5e4169510d8833f",
            "value": "Saving the dataset (15/15 shards): 100%"
          }
        },
        "7805177ac597487a843cc1fd8539bb0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13519bd423a04322a270979206269a1c",
            "placeholder": "​",
            "style": "IPY_MODEL_27b217d137af4a31a91fc2d215603be8",
            "value": "Map: 100%"
          }
        },
        "78ee14311c21415e8a11d1962689e49f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9522eb2c9543748c3ea5f394c91bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b1e1e56febe42389308ac7897c3cb66",
              "IPY_MODEL_7d7c8289369a4384920a9cfad85424ed",
              "IPY_MODEL_2a4a347e54144f15af60793a6ff65c07"
            ],
            "layout": "IPY_MODEL_8b6ae10b4fee4c55a0b48178b24e4921"
          }
        },
        "7b22f76c5e1d489386050b59a16b1cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c1371eea322443b8ce55833d928a0d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c425b3b58e94dbf94ca7cd5b5ac1397": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0d71003e47340dc84176f79e6fbd959",
              "IPY_MODEL_061ab3d77e304eb881fe43031268cee0",
              "IPY_MODEL_a598a7c7648f4d2b93eb4b04acaf655c"
            ],
            "layout": "IPY_MODEL_1a7d4c48d98c4d0690dc6f71155bfd76"
          }
        },
        "7d7c8289369a4384920a9cfad85424ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e7711979e504965a2a519dfbce3d938",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9cf215937d04d5da6fe81a7d0c70740",
            "value": 570
          }
        },
        "8012c352224b4d13bff2c28f3d889785": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85b3445c19464de6b174a9a20cb87932": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8652c73855c84183bf6086def164cb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_465ad038490d4b98b77e423f9069683f",
            "placeholder": "​",
            "style": "IPY_MODEL_bbafad8cc8f549b6870454a8dea3ee69",
            "value": " 440M/440M [00:04&lt;00:00, 156MB/s]"
          }
        },
        "869d2b5f5245434cafc4308238f45953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "873d7f423d1a47d7a16100d38105a402": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1e1e56febe42389308ac7897c3cb66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c669dc015840588ae94fffbf2bad9f",
            "placeholder": "​",
            "style": "IPY_MODEL_f5ff534e14db4d67886e4384ad2163ad",
            "value": "config.json: 100%"
          }
        },
        "8b3b8c0acf924626975e4cd67010b9b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b6ae10b4fee4c55a0b48178b24e4921": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8be5c0f4e2454ccfbf8793fa6717f176": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6a21cd8fb44c6987d4d93e73db7ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91e2c143213f413ca3c760e50380b345": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5067acc8b04dc78390e19d13f04dfe",
            "placeholder": "​",
            "style": "IPY_MODEL_2da496951a8447e693daaa77ee7bbd43",
            "value": " 6420790/6420790 [00:18&lt;00:00, 336497.97 examples/s]"
          }
        },
        "922b529e0b17405780f78ad3f227a96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa38594f75c140d184ca3480a0eb50a0",
              "IPY_MODEL_a38576acaee84bd7aa2ce1cc1048a0cc",
              "IPY_MODEL_d362eb8fc55f4fdba129e86ccc2fbd5f"
            ],
            "layout": "IPY_MODEL_9240fd64b90a47b2bcd0e3d428932402"
          }
        },
        "9240fd64b90a47b2bcd0e3d428932402": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95632537b0624076b08d245a97d26232": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a5067acc8b04dc78390e19d13f04dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c90385c7f3e4b95b65cccaed761adb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ca26d39193f4bf48893b3f897da3aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ea8ef3089a24c85a44dcb64a4671465",
              "IPY_MODEL_296c0e6ac8f84417ac617aceb3417b6f",
              "IPY_MODEL_64d7773e85784edfa2c3e13ad40b2b8b"
            ],
            "layout": "IPY_MODEL_c12fe4750e714b9a9558b0c5feea1308"
          }
        },
        "a38576acaee84bd7aa2ce1cc1048a0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0c9892b2aae4ce0a0c5f0198e9243a7",
            "max": 6420790,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_278f4ffee42141e583be84f5a8d440fa",
            "value": 6420790
          }
        },
        "a598a7c7648f4d2b93eb4b04acaf655c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_873d7f423d1a47d7a16100d38105a402",
            "placeholder": "​",
            "style": "IPY_MODEL_a5dc00c086194cdf8aa57476094e7cbd",
            "value": " 232k/232k [00:00&lt;00:00, 3.86MB/s]"
          }
        },
        "a5dc00c086194cdf8aa57476094e7cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64cc55ea6ae4c8b88f7e87073e64cee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c3fe81bae34f7ab859be4ef43b1608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa38594f75c140d184ca3480a0eb50a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85b3445c19464de6b174a9a20cb87932",
            "placeholder": "​",
            "style": "IPY_MODEL_55067fcbcc10451a9bf9104faad5097e",
            "value": "Filter: 100%"
          }
        },
        "ab24a9776b4344e6a8d5c98def359e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_039fe22528c043f1b5867ea9f812fd7c",
            "placeholder": "​",
            "style": "IPY_MODEL_5d7ae1d364ca4f5fa0a3ca0b2aabb7ec",
            "value": "model.safetensors: 100%"
          }
        },
        "b0c9892b2aae4ce0a0c5f0198e9243a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79b0d931eab44649667bcd1330e7a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f493f1a654e422fa5245023caa9623d",
            "placeholder": "​",
            "style": "IPY_MODEL_22e2c916762547c0b6add1676027ab32",
            "value": " 6420133/6420133 [31:21&lt;00:00, 3259.50 examples/s]"
          }
        },
        "b9192b928e2946baae4cd2dbc4580424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbafad8cc8f549b6870454a8dea3ee69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c12fe4750e714b9a9558b0c5feea1308": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c150c33eda4f4c96a003e6dca5353c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c641813b6ca3483aa7bb809dcbb1ac86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab24a9776b4344e6a8d5c98def359e4e",
              "IPY_MODEL_027ed76aa521457caf4f9ff18e9721d7",
              "IPY_MODEL_8652c73855c84183bf6086def164cb2b"
            ],
            "layout": "IPY_MODEL_22752e387a6342f78c404716df1fa916"
          }
        },
        "c8fda8a5d33843a5b1315ec756bb92b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf0b8afb8cdf4f72a7a3d125a5ac8941": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d035e4fc41cb4e0d96fe124d766ffb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d362eb8fc55f4fdba129e86ccc2fbd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be5c0f4e2454ccfbf8793fa6717f176",
            "placeholder": "​",
            "style": "IPY_MODEL_9c90385c7f3e4b95b65cccaed761adb1",
            "value": " 6420790/6420790 [00:30&lt;00:00, 215941.34 examples/s]"
          }
        },
        "e0501f70c205460cb5e4169510d8833f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0c669dc015840588ae94fffbf2bad9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e106371e198c4032ba4e1f807dbf3909": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef36478e6e2d4750be4d0890d1d594f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0d71003e47340dc84176f79e6fbd959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9eb11f3ea4479f9dc5527888e89e1e",
            "placeholder": "​",
            "style": "IPY_MODEL_d035e4fc41cb4e0d96fe124d766ffb21",
            "value": "vocab.txt: 100%"
          }
        },
        "f39d568282344207a05dac9635dcdf58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55b8b29080243d0b8a334befddbe42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00406e12f21041839b5ee0ea72f830fe",
              "IPY_MODEL_2b5aeeee9239473d93918a19d63d207f",
              "IPY_MODEL_0725a41da75f4039980dc64ce2e1074e"
            ],
            "layout": "IPY_MODEL_a64cc55ea6ae4c8b88f7e87073e64cee"
          }
        },
        "f5ff534e14db4d67886e4384ad2163ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7368e5e44494eb7a452acae2166c7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75bd038242c74f2ab696b7ca8e946405",
              "IPY_MODEL_18b0ac5688b543c5bdac9bfe17d98d3f",
              "IPY_MODEL_1d3ff95b2f86479392e5fde402b12d0a"
            ],
            "layout": "IPY_MODEL_fa8af006daeb44e0896c9b886220995d"
          }
        },
        "f9cf215937d04d5da6fe81a7d0c70740": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa8af006daeb44e0896c9b886220995d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffad06b5bd414676ba532957eea657ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad2e0be78d64296b1bf09957545e692",
            "max": 6420790,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b22f76c5e1d489386050b59a16b1cec",
            "value": 6420790
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
