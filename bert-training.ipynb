{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from torch import nn\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    BertConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) Load the crosswordqa_segmented dataset from disk\n",
    "dataset_path = \"crosswordqa_segmented\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {dataset_path}\")\n",
    "\n",
    "ds = load_from_disk(dataset_path)\n",
    "# print(ds)\n",
    "\n",
    "# We assume ds has at least a \"train\" split, with fields: \"clue\", \"segmented_answer\"\n",
    "\n",
    "# 3) Tokenizer for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_qa(batch):\n",
    "    # batch[\"clue\"] and batch[\"answer\"] are lists of strings\n",
    "    clue_encoding = tokenizer(\n",
    "        batch[\"clue\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=32\n",
    "    )\n",
    "    answer_encoding = tokenizer(\n",
    "        batch[\"answer\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=32\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"clue_input_ids\": clue_encoding[\"input_ids\"],\n",
    "        \"clue_attention_mask\": clue_encoding[\"attention_mask\"],\n",
    "        \"answer_input_ids\": answer_encoding[\"input_ids\"],\n",
    "        \"answer_attention_mask\": answer_encoding[\"attention_mask\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We'll create a new train_dataset with \"clue\" and \"answer\" fields,\n",
    "# copying from \"clue\" and \"segmented_answer\" in the ds[\"train\"].\n",
    "def map_to_train_fields(example):\n",
    "    return {\n",
    "        \"clue\": example[\"clue\"],\n",
    "        \"answer\": example[\"segmented_answer\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample before tokenization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6420790/6420790 [00:17<00:00, 361985.98 examples/s]\n",
      "Map: 100%|██████████| 6420133/6420133 [28:52<00:00, 3705.76 examples/s]\n",
      "Saving the dataset (15/15 shards): 100%|██████████| 6420133/6420133 [00:15<00:00, 413102.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Replace your processing code with:\n",
    "\n",
    "# We map the training split to unify field names for tokenize_qa\n",
    "\n",
    "train_dataset = ds[\"train\"].map(map_to_train_fields, batched=True)\n",
    "\n",
    "# Print a sample to debug\n",
    "print(\"Sample before tokenization:\")\n",
    "# print(train_dataset[0])\n",
    "\n",
    "# Try without batched mode first\n",
    "train_dataset = train_dataset.filter(lambda x: x[\"clue\"] and x[\"answer\"])\n",
    "train_dataset = train_dataset.map(tokenize_qa, batched=False)\n",
    "\n",
    "# print(train_dataset)\n",
    "train_dataset.save_to_disk(\"train_dataset_bert_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BiEncoderQA(nn.Module):\n",
    "    def _init_(self, model_name=\"bert-base-uncased\"):\n",
    "        super()._init_()  # Correct init for modern PyTorch\n",
    "        self.clue_encoder = BertModel.from_pretrained(model_name)\n",
    "        self.answer_encoder = BertModel.from_pretrained(model_name)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, \n",
    "                clue_input_ids, clue_attention_mask,\n",
    "                answer_input_ids, answer_attention_mask):\n",
    "        clue_outputs = self.clue_encoder(input_ids=clue_input_ids, attention_mask=clue_attention_mask)\n",
    "        clue_emb = clue_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        answer_outputs = self.answer_encoder(input_ids=answer_input_ids, attention_mask=answer_attention_mask)\n",
    "        answer_emb = answer_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        sim_matrix = torch.matmul(clue_emb, answer_emb.transpose(0, 1))\n",
    "        targets = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n",
    "        loss = self.loss_fct(sim_matrix, targets)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"sim_matrix\": sim_matrix,\n",
    "            \"clue_emb\": clue_emb,\n",
    "            \"answer_emb\": answer_emb,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BiEncoderQA.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4) Initialize the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mBiEncoderQA\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbert-base-uncased\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 5) Set up TrainingArguments\u001b[39;00m\n\u001b[32m      5\u001b[39m training_args = TrainingArguments(\n\u001b[32m      6\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./bi_encoder_qa\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     num_train_epochs=\u001b[32m3\u001b[39m,                \u001b[38;5;66;03m# adjust for real training\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     fp16=torch.cuda.is_available(),  \u001b[38;5;66;03m# Use fp16 if GPU supports it\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Restricted_project\\Andi_Mandi_Shandi_iske_andar_gya_toh\\BKL_aa_hi_gye_naa\\toh_suno_TMKC\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:488\u001b[39m, in \u001b[36mModule.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    483\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() got an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m     )\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m given\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m     )\n\u001b[32m    493\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[33;03mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[33;03mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[32m    496\u001b[39m \u001b[33;03mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[33;03msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__setattr__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: BiEncoderQA.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# 4) Initialize the model\n",
    "model = BiEncoderQA(\"bert-base-uncased\")\n",
    "\n",
    "# 5) Set up TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bi_encoder_qa\",\n",
    "    num_train_epochs=3,                # adjust for real training\n",
    "    per_device_train_batch_size=8,     # tune for GPU RAM\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",  # or \"steps\"/\"epoch\" if you have a val set\n",
    "    fp16=torch.cuda.is_available(),  # Use fp16 if GPU supports it\n",
    ")\n",
    "\n",
    "# 6) Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7) Train the model (Optional)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8) Inference with FAISS (like the original example)\n",
    "#    We assume you built a large answer_set file (answer_set_segmented.txt).\n",
    "#    We'll just show how to embed & index it.\n",
    "\n",
    "answer_file = \"answer_set_segmented.txt\"\n",
    "if not os.path.exists(answer_file):\n",
    "    print(f\"❌ {answer_file} not found. Please provide a valid path.\")\n",
    "else:\n",
    "    # Load your large answer set from file\n",
    "    with open(answer_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_answers = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"✅ Loaded {len(file_answers)} answers from {answer_file}\")\n",
    "\n",
    "    # Function to compute embeddings for answers\n",
    "    def compute_answer_embeddings(answer_list, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        model.eval()\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for ans in answer_list:\n",
    "                encoded = tokenizer(\n",
    "                    ans,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=32\n",
    "                ).to(device)\n",
    "                outputs = model.answer_encoder(**encoded)\n",
    "                emb = outputs.last_hidden_state[:, 0, :]\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "        # shape: (num_answers, hidden_dim)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Compute embeddings\n",
    "    answer_embeddings = compute_answer_embeddings(file_answers, model, tokenizer, device=device)\n",
    "    dim = answer_embeddings.shape[1]\n",
    "\n",
    "    # Build FAISS index for cosine similarity\n",
    "    faiss.normalize_L2(answer_embeddings)\n",
    "    faiss_index = faiss.IndexFlatIP(dim)\n",
    "    faiss_index.add(answer_embeddings)\n",
    "    print(f\"✅ Built FAISS index with {faiss_index.ntotal} embeddings.\")\n",
    "\n",
    "# Inference function\n",
    "    def answer_for_clue(clue, model, tokenizer, faiss_index, answer_list, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", k=3):\n",
    "        model.eval()\n",
    "        encoded = tokenizer(clue, return_tensors=\"pt\", truncation=True, padding=True, max_length=32).to(device)\n",
    "        with torch.no_grad():\n",
    "            clue_output = model.clue_encoder(**encoded)\n",
    "            clue_emb = clue_output.last_hidden_state[:, 0, :]\n",
    "            clue_emb = clue_emb.cpu().numpy()\n",
    "        # Normalize clue for cosine\n",
    "        faiss.normalize_L2(clue_emb)\n",
    "        distances, indices = faiss_index.search(clue_emb, k)\n",
    "        candidates = [answer_list[idx] for idx in indices[0]]\n",
    "        return candidates\n",
    "\n",
    "    # Test\n",
    "    test_clue = \"What is the capital of France?\"\n",
    "    predicted_answers = answer_for_clue(test_clue, model, tokenizer, faiss_index, file_answers, device=device, k=3)\n",
    "    print(\"Clue:\", test_clue)\n",
    "    print(\"Candidate Answers:\", predicted_answers)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
